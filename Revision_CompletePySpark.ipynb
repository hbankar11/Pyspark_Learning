{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4fyDiGiXMUyZg1hxaXIG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbankar11/Pyspark_Learning/blob/main/Revision_CompletePySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZIt4i3jfS1Z",
        "outputId": "81c694c6-f3a1-4fc6-afba-61813e4ef41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Just Checking PySpark is Installed or not\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "BUUaHRD0f4kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Spark Session\n",
        "spark_session = SparkSession.builder.appName(\"Revision_CompletePySpark\").getOrCreate()"
      ],
      "metadata": {
        "id": "g_Iw0d7YhhPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create SparkContext in PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_session = SparkSession.builder.appName(\"Revision_PySpark\").getOrCreate()\n",
        "print(spark_session.sparkContext)\n",
        "print(\"Spark App Name : \" + spark_session.sparkContext.appName)"
      ],
      "metadata": {
        "id": "HmUp0sLlh-vJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6da813-cbdc-4549-b01e-6071b667d82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=Revision_CompletePySpark>\n",
            "Spark App Name : Revision_CompletePySpark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop PySpark SparkContext\n",
        "#When PySpark executes this statement, it logs the message INFO SparkContext: Successfully stopped SparkContext to console or to a log file.\n",
        "spark_session.sparkContext.stop()"
      ],
      "metadata": {
        "id": "Dty405JtkM15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create RDD using parallelize\n",
        "data = [1,2,3,4,5,6,7,8,9,10]\n",
        "rdd = spark_session.sparkContext.parallelize(data)\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wTnOT6Ym6wg",
        "outputId": "cf2e450b-7b83-4107-eda3-0115ea176ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create RDD using sparkContext.textFile()\n",
        "rdd2 = spark_session.sparkContext.textFile(\"/path/textFile.txt\")\n"
      ],
      "metadata": {
        "id": "nFnL_J1WpXsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create RDD using sparkContext.wholeTextFiles()\n",
        "rdd3 = spark_session.sparkContext.wholeTextFiles(\"<path>\")"
      ],
      "metadata": {
        "id": "0w6uNPCwpnNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create empty RDD using sparkContext.emptyRDD\n",
        "emptyRDD = spark_session.sparkContext.emptyRDD"
      ],
      "metadata": {
        "id": "oIy_Ycamp4D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating empty RDD with partition\n",
        "emptyRDDPartition = spark_session.sparkContext.parallelize([],10)"
      ],
      "metadata": {
        "id": "-6c-4qsPqXEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD Partitions\n",
        "#Get Partition count\n",
        "print(\"Initial partition count : \" + str(rdd.getNumPartitions()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwn4nQlOqmVY",
        "outputId": "2b376830-e732-4901-e4b9-b8742ade21cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial partition count : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set partition Manually\n",
        "rddPartition = spark_session.sparkContext.parallelize([1,2,3],10)\n",
        "rddPartition.collect()\n",
        "rddPartition.getNumPartitions() #10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctD_mX26rPoy",
        "outputId": "2b77dc05-e07f-4228-ff64-653509f55b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Repartition and coalesce\n",
        "reparRdd = rdd.repartition(4)\n",
        "#print(\"re-partition count : \"+str(reparRdd.getNumPartitions()))\n",
        "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
        "\n",
        "coalRdd = rdd.coalesce(2)\n",
        "print(\"coalesce count:\"+str(coalRdd.getNumPartitions()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk8dgIshSa4W",
        "outputId": "8e8f88e9-28e4-4be3-98e4-6867cf01b3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-partition count:4\n",
            "coalesce count:2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatMap() : flattens all those sequences into a single RDD\n",
        "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n"
      ],
      "metadata": {
        "id": "Zr4gIMaGTdFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#map() : It allows you to apply a function to each element of an RDD, producing a new RDD with the transformed elements.\n",
        "# Add a new element with value 1 to each word\n",
        "rdd3 = rdd2.map(lambda x: (x,1))\n"
      ],
      "metadata": {
        "id": "3z6iIsBZUila"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reduceByKey() : combines the values associated with each key using the provided function.\n",
        "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n"
      ],
      "metadata": {
        "id": "cswM5wAoVVkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sortByKey() : transformation arranges the elements of an RDD based on their keys.\n",
        "# Using sortByKey()\n",
        "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
        "\n",
        "# Print rdd5 result to console\n",
        "print(rdd5.collect())"
      ],
      "metadata": {
        "id": "gbdwKcbUWsy3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9febb1e8-0220-4928-b4f2-bf8f53081db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 12) (132042cd4aa0 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3029351147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sortByKey() : transformation arranges the elements of an RDD based on their keys.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Using sortByKey()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrdd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print rdd5 result to console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \"\"\"\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \"\"\"\n\u001b[0;32m-> 2291\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2292\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 12) (132042cd4aa0 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD action:\n",
        "#1: count() => Returns the number of records in an RDD\n",
        "print(\"Count : \"+str(rdd.count()))\n",
        "\n",
        "#2: first() => Returns the first record.\n",
        "firstRec = rdd.first()\n",
        "print(\"First Record : \"+str(firstRec))\n",
        "\n",
        "#3:max() => Returns maximum record\n",
        "datMax = rdd.max()\n",
        "print(\"Maximum Record : \"+str(datMax))\n",
        "\n",
        "#4: reduce() => Reduces the records to single, we can use this to count or sum.\n",
        "#totalWordCount = rdd.reduce(lambda a,b: (a[0]+b[0],a[1]))\n",
        "#print(\"dataReduce Record : \"+str(totalWordCount[0]))\n",
        "\n",
        "#5: take() =>  Returns the record specified as an argument.\n",
        "data = rdd.take(3)\n",
        "print(\"Element at position 3 :\"+str(data))\n",
        "\n",
        "#6: saveAsTextFile() => Using saveAsTestFile action, we can write the RDD to a text file.\n",
        "rdd.saveAsTextFile(\"/tmp/wordCount\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF67bdw-XsG1",
        "outputId": "de2e44b4-29ef-4c74-c8f4-08f6b0b691be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count : 10\n",
            "First Record : 1\n",
            "Maximum Record : 10\n",
            "Element at position 3 :[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating RDD from DataFrame and vice-versa\n",
        "# Converts RDD to DataFrame\n",
        "dfFromRDD1 = rdd.toDF()\n",
        "\n",
        "# Converts RDD to DataFrame with column names\n",
        "dfFromRDD2 = rdd.toDF(\"col1\",\"col2\")\n",
        "\n",
        "# using createDataFrame() - Convert DataFrame to RDD\n",
        "df = spark_session.createDataFrame(rdd).toDF(\"col1\",\"col2\")\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "rdd = df.rdd\n"
      ],
      "metadata": {
        "id": "4yOGdCw6fwZf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "1b042d17-09c7-4785-b11d-9f6dab0bc282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-229215165.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Creating RDD from DataFrame and vice-versa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Converts RDD to DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfFromRDD1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Converts RDD to DataFrame with column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             schema = _infer_schema(\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_INFER_SCHEMA_FOR_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataframe\n",
        "#1: Create empty RDD in Pyspark\n",
        "emptyRDD = spark_session.sparkContext.emptyRDD()\n",
        "print(emptyRDD)\n",
        "\n",
        "#2: Alternative method to create empty RDD\n",
        "emptyRDD1 = spark_session.sparkContext.parallelize([])\n",
        "print(emptyRDD1)\n",
        "\n",
        "#3:Create Empty DataFrame with Schema (StructType)\n",
        "from pyspark.sql.types import StructField,StructType,StringType\n",
        "schema = StructType([\n",
        "    StructField('firstname',StringType(),True),\n",
        "    StructField('middlename',StringType(),True),\n",
        "    StructField('lastname',StringType(),True)\n",
        "])\n",
        "\n",
        "emptyDF = spark_session.createDataFrame(emptyRDD,schema)\n",
        "emptyDF.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFUIqbtsgJp1",
        "outputId": "074fbb48-ce5e-482f-93a3-f4912e7c863e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmptyRDD[26] at emptyRDD at NativeMethodAccessorImpl.java:0\n",
            "ParallelCollectionRDD[27] at readRDDFromFile at PythonRDD.scala:289\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Empty RDD to DataFrame\n",
        "df = emptyRDD.toDF(schema)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKYWeEGCrFVR",
        "outputId": "234bf8a5-2355-48ee-bcf8-9657d83c544b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Empty DataFrame with Schema.\n",
        "df = spark_session.createDataFrame([],schema)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pIz-MJ_rycx",
        "outputId": "dc1ed21b-2491-4433-bc57-191d259f0dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Empty DataFrame without Schema (no columns)\n",
        "df = spark_session.createDataFrame([],StructType([]))\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "---p5pVOsGVF",
        "outputId": "2474fafd-6193-4190-f955-88d7b18c6824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create pyspark RDD\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark_session.sparkContext.parallelize(dept)\n",
        "rdd.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqc6g5_DscIL",
        "outputId": "bd866370-ae00-4f54-933a-43317c4d1bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Pyspark rdd to dataframe\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vT_WzkDtN9g",
        "outputId": "ddc6c265-b9f7-4291-ab25-9860892d6be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-QSiHffthnu",
        "outputId": "ac29185b-ab83-4c6f-e24e-8f648bee6e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Pyspark createDataFrame() function\n",
        "deptDF = spark_session.createDataFrame(rdd,schema=deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z07XiuEVuSSG",
        "outputId": "71e61c4f-4cce-4493-e681-fe5a676f97d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using createDataFrame() with StructType schema\n",
        "from pyspark.sql.types import StringType,StructType,StructField,StringType,IntegerType\n",
        "\n",
        "deptSchema = StructType([\n",
        "    StructField(\"dept_name\",StringType(),True),\n",
        "    StructField(\"dept_id\",IntegerType(),True),\n",
        "])\n",
        "deptDF1 = spark_session.createDataFrame(rdd,schema=deptSchema)\n",
        "deptDF1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFrU2CWSvHmz",
        "outputId": "ce27bf8d-b058-46a4-b9ab-773f18a0b76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pyspark Dataframe to Pandas Dataframe\n",
        "# Consider below pyspark dataframe\n",
        "\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
        "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
        "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
        "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
        "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
        "\n",
        "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
        "pysparkDF = spark_session.createDataFrame(data = data, schema = columns)\n",
        "#pysparkDF.printSchema()\n",
        "#pysparkDF.show(truncate=False)\n",
        "\n",
        "#Convert above pyspark dataframe to Pandas\n",
        "pandasDF = pysparkDF.toPandas()\n",
        "print(pandasDF)"
      ],
      "metadata": {
        "id": "d5yplcqUyOgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24faf31f-41c0-4bd1-a208-a8e96ecbf2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  first_name middle_name last_name    dob gender  salary\n",
            "0      James                 Smith  36636      M   60000\n",
            "1    Michael        Rose            40288      M   70000\n",
            "2     Robert              Williams  42114         400000\n",
            "3      Maria        Anne     Jones  39192      F  500000\n",
            "4        Jen        Mary     Brown             F       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark show() – Display DataFrame Contents in Table\n",
        "# Default - displays 20 rows and\n",
        "# 20 charactes from column value\n",
        "df.show()\n",
        "\n",
        "#Display full column contents\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Display 2 rows and full column contents\n",
        "df.show(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)"
      ],
      "metadata": {
        "id": "uhMgZxEPPkse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Below example\n",
        "columns = [\"Seqno\",\"Quote\"]\n",
        "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
        "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
        "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
        "    (\"4\", \"Be cool.\")]\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "df.show()\n",
        "df.show(truncate=False)\n",
        "df(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6K2QY-qQCqI",
        "outputId": "c6b684b6-e3e2-4b2e-ff02-4465f789a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------------------------------------------------------------------+\n",
            "|Seqno|Quote                                                                        |\n",
            "+-----+-----------------------------------------------------------------------------+\n",
            "|1    |Be the change that you wish to see in the world                              |\n",
            "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
            "|3    |The purpose of our lives is to be happy.                                     |\n",
            "|4    |Be cool.                                                                     |\n",
            "+-----+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark StructType & StructField Explained with Examples\n",
        "#Using PySpark StructType & StructField with DataFrame\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "  ]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"First_Name\",StringType(),True),\n",
        "    StructField(\"Middle_Name\",StringType(),True),\n",
        "    StructField(\"Last_Name\",StringType(),True),\n",
        "    StructField(\"ID\",StringType(),True),\n",
        "    StructField(\"Gender\",StringType(),True),\n",
        "    StructField(\"Salary\",IntegerType(),True)\n",
        "\n",
        "])\n",
        "\n",
        "df = spark_session.createDataFrame(data,schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ONbC5VlkSLfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1636b810-bc06-45d2-a046-af4f7b724b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- First_Name: string (nullable = true)\n",
            " |-- Middle_Name: string (nullable = true)\n",
            " |-- Last_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "+----------+-----------+---------+-----+------+------+\n",
            "|First_Name|Middle_Name|Last_Name|ID   |Gender|Salary|\n",
            "+----------+-----------+---------+-----+------+------+\n",
            "|James     |           |Smith    |36636|M     |3000  |\n",
            "|Michael   |Rose       |         |40288|M     |4000  |\n",
            "|Robert    |           |Williams |42114|M     |4000  |\n",
            "|Maria     |Anne       |Jones    |39192|F     |4000  |\n",
            "|Jen       |Mary       |Brown    |     |F     |-1    |\n",
            "+----------+-----------+---------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining schema using nested StructType\n",
        "from pyspark.sql.types import StructType,StructField,StringType\n",
        "structureData = [\n",
        "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
        "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
        "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\",StructType([\n",
        "        StructField(\"FirstName\",StringType(),True),\n",
        "        StructField(\"MiddleName\",StringType(),True),\n",
        "        StructField(\"LastName\",StringType(),True),]),True),\n",
        "    StructField(\"ID\",StringType(),True),\n",
        "    StructField(\"Gender\",StringType(),True),\n",
        "    StructField(\"Salary\",StringType(),True)\n",
        "])\n",
        "\n",
        "df = spark_session.createDataFrame(structureData,schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecYN3Bq_Cfii",
        "outputId": "7772d589-e7ac-4d11-d7dc-56adbd9fff14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: struct (nullable = true)\n",
            " |    |-- FirstName: string (nullable = true)\n",
            " |    |-- MiddleName: string (nullable = true)\n",
            " |    |-- LastName: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Salary: string (nullable = true)\n",
            "\n",
            "+--------------------+-----+------+------+\n",
            "|Name                |ID   |Gender|Salary|\n",
            "+--------------------+-----+------+------+\n",
            "|{James, , Smith}    |36636|M     |3100  |\n",
            "|{Michael, Rose, }   |40288|M     |4300  |\n",
            "|{Robert, , Williams}|42114|M     |1400  |\n",
            "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
            "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
            "+--------------------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Create Column Class Object\n",
        "from pyspark.sql.functions import lit\n",
        "colObj = lit(\"sparkbyexamples.com\")"
      ],
      "metadata": {
        "id": "Hyvxlhm2K3V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "data=[(\"James\",23),(\"Ann\",40)]\n",
        "df=spark_session.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
        "df.printSchema()\n",
        "#root\n",
        "# |-- name.fname: string (nullable = true)\n",
        "# |-- gender: long (nullable = true)\n",
        "\n",
        "# Using DataFrame object (df)\n",
        "df.select(df.gender).show(truncate=False)\n",
        "df.select(df[\"gender\"]).show(truncate=False)\n",
        "\n",
        "#Accessing column name with dot operator\n",
        "df.select(df[\"`name.fname`\"]).show(truncate=False)\n",
        "\n",
        "#Using SQL col() function\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"gender\")).show(truncate=False)\n",
        "\n",
        "#Accessing column name with dot (with backticks)\n",
        "df.select(col(\"`name.fname`\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEU8LV8CMDkN",
        "outputId": "723d5666-c813-4dbc-d0fc-585eb18cca4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name.fname: string (nullable = true)\n",
            " |-- gender: long (nullable = true)\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|23    |\n",
            "|40    |\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|23    |\n",
            "|40    |\n",
            "+------+\n",
            "\n",
            "+----------+\n",
            "|name.fname|\n",
            "+----------+\n",
            "|James     |\n",
            "|Ann       |\n",
            "+----------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|23    |\n",
            "|40    |\n",
            "+------+\n",
            "\n",
            "+----------+\n",
            "|name.fname|\n",
            "+----------+\n",
            "|James     |\n",
            "|Ann       |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create DataFrame with struct using Row class\n",
        "from pyspark.sql import Row\n",
        "\n",
        "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
        "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
        "\n",
        "df = spark_session.createDataFrame(data)\n",
        "df.printSchema()\n",
        "\n",
        "#Access struct column\n",
        "df.select(df.prop.hair).show(truncate=False)\n",
        "df.select(df[\"prop.hair\"]).show(truncate=False)\n",
        "df.select(col(\"prop.hair\")).show(truncate=False)\n",
        "\n",
        "#Access all columns from struct\n",
        "df.select(col(\"prop.*\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSGbITT3Owlg",
        "outputId": "9505eb75-9272-4491-ad26-284df633f099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- prop: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            "\n",
            "+---------+\n",
            "|prop.hair|\n",
            "+---------+\n",
            "|black    |\n",
            "|grey     |\n",
            "+---------+\n",
            "\n",
            "+-----+\n",
            "|hair |\n",
            "+-----+\n",
            "|black|\n",
            "|grey |\n",
            "+-----+\n",
            "\n",
            "+-----+\n",
            "|hair |\n",
            "+-----+\n",
            "|black|\n",
            "|grey |\n",
            "+-----+\n",
            "\n",
            "+-----+-----+\n",
            "|hair |eye  |\n",
            "+-----+-----+\n",
            "|black|blue |\n",
            "|grey |black|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark_session.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "#Arthmetic operations\n",
        "df.select(col(\"col1\") + col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") - col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") * col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") / col(\"col2\")).show(truncate=False)\n",
        "\n",
        "df.select(col(\"col1\") % col(\"col2\")).show(truncate=False)\n",
        "\n",
        "df.select(col(\"col1\") == col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") > col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") < col(\"col2\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZznNH8_Swhn",
        "outputId": "35e86a26-b0b1-4341-811f-dd27cc3be030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|(col1 + col2)|\n",
            "+-------------+\n",
            "|102          |\n",
            "|203          |\n",
            "|304          |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 - col2)|\n",
            "+-------------+\n",
            "|98           |\n",
            "|197          |\n",
            "|296          |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 * col2)|\n",
            "+-------------+\n",
            "|200          |\n",
            "|600          |\n",
            "|1200         |\n",
            "+-------------+\n",
            "\n",
            "+-----------------+\n",
            "|(col1 / col2)    |\n",
            "+-----------------+\n",
            "|50.0             |\n",
            "|66.66666666666667|\n",
            "|75.0             |\n",
            "+-----------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 % col2)|\n",
            "+-------------+\n",
            "|0            |\n",
            "|2            |\n",
            "|0            |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 = col2)|\n",
            "+-------------+\n",
            "|false        |\n",
            "|false        |\n",
            "|false        |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 > col2)|\n",
            "+-------------+\n",
            "|true         |\n",
            "|true         |\n",
            "|true         |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 < col2)|\n",
            "+-------------+\n",
            "|false        |\n",
            "|false        |\n",
            "|false        |\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr,col\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#alias() – Set’s name to Column\n",
        "df.select(df.fname.alias(\"First_Name\"), \\\n",
        "          df.lname.alias(\"Last_Name\")).show(truncate=False)\n",
        "\n",
        "#Another Example\n",
        "df.select(expr(\"fname ||' '|| lname\").alias(\"Full_Name\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0hHcEJS7jBd",
        "outputId": "42d1ba42-9bba-4f22-fd13-8345f9f3c1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|First_Name|Last_Name|\n",
            "+----------+---------+\n",
            "|James     |Bond     |\n",
            "|Ann       |Varsa    |\n",
            "|Tom Cruise|XXX      |\n",
            "|Tom Brand |NULL     |\n",
            "+----------+---------+\n",
            "\n",
            "+--------------+\n",
            "|Full_Name     |\n",
            "+--------------+\n",
            "|James Bond    |\n",
            "|Ann Varsa     |\n",
            "|Tom Cruise XXX|\n",
            "|NULL          |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asc() & desc() – Sort the DataFrame columns by Ascending or Descending order.\n",
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()\n"
      ],
      "metadata": {
        "id": "gngBCHNG-sul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1b25ee-6c35-4282-ca2c-5d3d8f7b1e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|       Ann|Varsa|200|     F|\n",
            "|     James| Bond|100|  NULL|\n",
            "| Tom Brand| NULL|400|     M|\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "+----------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "| Tom Brand| NULL|400|     M|\n",
            "|     James| Bond|100|  NULL|\n",
            "|       Ann|Varsa|200|     F|\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cast() & astype() – Used to convert the data Type.\n",
        "df.select(df.fname,df.id.cast('int')).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtlU3an_FJf",
        "outputId": "5b6e3a1b-b5ef-4275-cee1-be9d70677173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- fname: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#between() – Returns a Boolean expression when a column values in between lower and upper bound.\n",
        "df.filter(df.id.between(100,300)).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2jdZtp__tf",
        "outputId": "ef648812-4302-401e-8087-f724bb2f4f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---+------+\n",
            "|fname|lname|id |gender|\n",
            "+-----+-----+---+------+\n",
            "|James|Bond |100|NULL  |\n",
            "|Ann  |Varsa|200|F     |\n",
            "+-----+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contains() -\n",
        "df.filter(df.fname.contains(\"Cruise\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "TnIJz7KnAhH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71713dd9-3450-4d68-ae9a-6ec3d0658be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# startsWith and endsWith()\n",
        "df.filter(df.fname.startswith(\"T\")).show(truncate=False)\n",
        "df.filter(df.fname.endswith(\"Cruise\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfzSG1UJSms8",
        "outputId": "ec898cee-9bed-4a66-c07c-a30155b34ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "|Tom Brand |NULL |400|M     |\n",
            "+----------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# isNull & isNotNull() – Checks if the DataFrame column has NULL or non NULL values.\n",
        "df.filter(df.lname.isNull()).show(truncate=False)\n",
        "df.filter(df.lname.isNotNull()).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M21m_hZUaAMd",
        "outputId": "3f4cc377-e8ab-496b-e1bf-9e7c26bb3e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+---+------+\n",
            "|fname    |lname|id |gender|\n",
            "+---------+-----+---+------+\n",
            "|Tom Brand|NULL |400|M     |\n",
            "+---------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|James     |Bond |100|NULL  |\n",
            "|Ann       |Varsa|200|F     |\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# like() & rlike() – Similar to SQL LIKE expression\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.fname.like(\"%om%\")) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq86a9JEa6eJ",
        "outputId": "ef153807-0caf-41f7-d421-5f245cb3f7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+\n",
            "|fname     |lname|id |\n",
            "+----------+-----+---+\n",
            "|Tom Cruise|XXX  |400|\n",
            "|Tom Brand |NULL |400|\n",
            "+----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# substr() – Returns a Column after getting sub string from the Column\n",
        "df.select(df.fname.substr(1,2).alias(\"substr\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uFDQGgubU8J",
        "outputId": "d663666a-9362-44a8-865e-6c2c5f8fa508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|substr|\n",
            "+------+\n",
            "|Ja    |\n",
            "|An    |\n",
            "|To    |\n",
            "|To    |\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when() & otherwise() – It is similar to SQL Case When, executes sequence of expressions until it matches the condition and returns a value when match.\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender == 'M','Male') \\\n",
        "                           .when(df.gender == 'F','Female') \\\n",
        "                           .when(df.gender == None, \"\") \\\n",
        "                           .otherwise(df.gender).alias(\"gender\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7DaE0oPgj8m",
        "outputId": "bf1b067c-09f8-4d69-f59d-5a167cd73f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+\n",
            "|fname     |lname|gender|\n",
            "+----------+-----+------+\n",
            "|James     |Bond |NULL  |\n",
            "|Ann       |Varsa|Female|\n",
            "|Tom Cruise|XXX  |      |\n",
            "|Tom Brand |NULL |Male  |\n",
            "+----------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# isin() – Check if value presents in a List.\n",
        "li = ['100','200']\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.id.isin(li)) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uedpshPpjc3T",
        "outputId": "9149372e-82c9-4903-b5bd-1828bc6fda63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---+\n",
            "|fname|lname|id |\n",
            "+-----+-----+---+\n",
            "|James|Bond |100|\n",
            "|Ann  |Varsa|200|\n",
            "+-----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getField() – To get the value by key from MapType column and by struct child name from StructType column\n",
        "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
        "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
        "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
        "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
        "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
        "\n",
        "schema = StructType([\n",
        "        StructField('name', StructType([\n",
        "            StructField('fname', StringType(), True),\n",
        "            StructField('lname', StringType(), True)])),\n",
        "        StructField('languages', ArrayType(StringType()),True),\n",
        "        StructField('properties', MapType(StringType(),StringType()),True)\n",
        "])\n",
        "\n",
        "df = spark_session.createDataFrame(data,schema)\n",
        "df.select(df.properties.getField(\"hair\")).show(truncate=False)\n",
        "\n",
        "df.select(df.name.getField(\"fname\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "n_HwNgkPkCLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6604d1-beb4-4bb3-df12-36072d4813e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|properties[hair]|\n",
            "+----------------+\n",
            "|black           |\n",
            "|brown           |\n",
            "|red             |\n",
            "|black           |\n",
            "+----------------+\n",
            "\n",
            "+----------+\n",
            "|name.fname|\n",
            "+----------+\n",
            "|James     |\n",
            "|Ann       |\n",
            "|Tom Cruise|\n",
            "|Tom Brand |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getItem() – To get the value by index from MapType or ArrayTupe & ny key for MapType column.\n",
        "\n",
        "#getItem() used with ArrayType\n",
        "df.select(df.languages.getItem(1)).show()\n",
        "\n",
        "#getItem() used with MapType\n",
        "df.select(df.properties.getItem(\"hair\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSYG7uJ_Wi1T",
        "outputId": "95008852-ce58-4f33-917d-91b7680137dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|languages[1]|\n",
            "+------------+\n",
            "|          C#|\n",
            "|      Python|\n",
            "|       Scala|\n",
            "|        Ruby|\n",
            "+------------+\n",
            "\n",
            "+----------------+\n",
            "|properties[hair]|\n",
            "+----------------+\n",
            "|           black|\n",
            "|           brown|\n",
            "|             red|\n",
            "|           black|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select-columns-from-pyspark-dataframe\n",
        "# consider below example\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "]\n",
        "\n",
        "# Column names\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "df.printSchema()\n",
        "\n",
        "#1: Select Single & Multiple Columns From PySpark\n",
        "df.select(\"firstname\",\"lastname\").show(truncate=False)\n",
        "df.select(df.firstname,df.lastname).show(truncate=False)\n",
        "df.select(df[\"firstname\"],df[\"lastname\"]).show(truncate=False)\n",
        "\n",
        "#1.1 using col()\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"firstname\"),col(\"lastname\")).show(truncate=False)\n",
        "\n",
        "# Select columns by regular expression\n",
        "df.select(df.colRegex(\"`^.*name*`\")).show()\n",
        "\n",
        "#2. Select All Columns From List\n",
        "df.select(*columns).show(truncate=False)\n",
        "\n",
        "#select All columns\n",
        "df.select([col for col in df.columns]).show(truncate=False)\n",
        "df.select(\"*\").show(truncate=False)\n",
        "\n",
        "#3. Select Columns by Index\n",
        "#3.1 Selects first 3 columns and top 3 rows\n",
        "df.select(df.columns[:3]).show(3,truncate=False)\n",
        "\n",
        "#3.2 Selects columns 2 to 4  and top 3 rows\n",
        "df.select(df.columns[2:4]).show(3,truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ghKQQFcLKF",
        "outputId": "f8e44449-ea48-4081-a7eb-7277a893595b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+\n",
            "|firstname|lastname|country|\n",
            "+---------+--------+-------+\n",
            "|James    |Smith   |USA    |\n",
            "|Michael  |Rose    |USA    |\n",
            "|Robert   |Williams|USA    |\n",
            "+---------+--------+-------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------+-----+\n",
            "|country|state|\n",
            "+-------+-----+\n",
            "|USA    |CA   |\n",
            "|USA    |NY   |\n",
            "|USA    |CA   |\n",
            "+-------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.4 Select Nested Struct Columns from PySpark\n",
        "from pyspark.sql.types import StructType,StructField,StringType\n",
        "# Create DataFrame with nested columns\n",
        "data = [\n",
        "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\",StructType([\n",
        "        StructField(\"Firstname\",StringType(),True),\n",
        "        StructField(\"Middlename\",StringType(),True),\n",
        "        StructField(\"Lastname\",StringType(),True),\n",
        "    ]),True),\n",
        "    StructField(\"State\",StringType(),True),\n",
        "    StructField(\"gender\",StringType(),True),\n",
        "])\n",
        "\n",
        "df2 = spark_session.createDataFrame(data,schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "#1: To select struct column name\n",
        "df2.select(\"name\").show(truncate=False)\n",
        "\n",
        "#2: In order to select the specific column from a nested struct, you need to explicitly qualify the nested struct column name.\n",
        "df2.select(\"Name.Firstname\",\"Name.Lastname\").show(truncate=False)\n",
        "\n",
        "#3: To get all columns from the struct column, specify name.*\n",
        "df2.select(\"Name.*\").show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C23_1NoppPwi",
        "outputId": "19b9d2a1-f2ac-487e-9bf2-f46e1c166f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: struct (nullable = true)\n",
            " |    |-- Firstname: string (nullable = true)\n",
            " |    |-- Middlename: string (nullable = true)\n",
            " |    |-- Lastname: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|Name                  |State|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n",
            "+----------------------+\n",
            "|name                  |\n",
            "+----------------------+\n",
            "|{James, NULL, Smith}  |\n",
            "|{Anna, Rose, }        |\n",
            "|{Julia, , Williams}   |\n",
            "|{Maria, Anne, Jones}  |\n",
            "|{Jen, Mary, Brown}    |\n",
            "|{Mike, Mary, Williams}|\n",
            "+----------------------+\n",
            "\n",
            "+---------+--------+\n",
            "|Firstname|Lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Anna     |        |\n",
            "|Julia    |Williams|\n",
            "|Maria    |Jones   |\n",
            "|Jen      |Brown   |\n",
            "|Mike     |Williams|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+----------+--------+\n",
            "|Firstname|Middlename|Lastname|\n",
            "+---------+----------+--------+\n",
            "|James    |NULL      |Smith   |\n",
            "|Anna     |Rose      |        |\n",
            "|Julia    |          |Williams|\n",
            "|Maria    |Anne      |Jones   |\n",
            "|Jen      |Mary      |Brown   |\n",
            "|Mike     |Mary      |Williams|\n",
            "+---------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Collect() – Retrieve data from DataFrame\n",
        "# Consider below example:\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "df = spark_session.createDataFrame(dept,deptColumns)\n",
        "df.collect()\n",
        "\n",
        "#Return value of first row, first column\n",
        "df.collect()[0][0]\n",
        "\n",
        "#returns the first element in an array (1st row).\n",
        "df.collect()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys65znf0ybWy",
        "outputId": "373ef088-671a-48e0-f06f-2d61c6c84c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(dept_name='Finance', dept_id=10)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark withColumn() Usage with Examples\n",
        "# consider below example\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#1: Change DataType using PySpark withColumn()\n",
        "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).printSchema()\n",
        "\n",
        "#2: Update The Value of an Existing Column\n",
        "df.withColumn(\"salary\",col(\"salary\")*100).show(truncate=False)\n",
        "\n",
        "#3: Create a Column from an Existing\n",
        "df.withColumn(\"copiedColumn\",col(\"salary\")* 1).show(truncate=False)\n",
        "\n",
        "#4: Add a New Column using withColumn()\n",
        "from pyspark.sql.functions import lit\n",
        "df.withColumn(\"country\",lit(\"USA\")) \\\n",
        "  .withColumn(\"AnotherValue\",lit(\"another_value\")) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "#5. Rename Column Name\n",
        "df.withColumnRenamed(\"gender\",\"sex\").printSchema()\n",
        "\n",
        "#6. Drop Column From PySpark DataFrame\n",
        "df.drop(\"gender\").printSchema()\n",
        "\n"
      ],
      "metadata": {
        "id": "bnfWUT-rgSZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4b4b04-af0e-4812-8a9d-afc351ea9553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|James    |          |Smith   |1991-04-01|M     |300000|\n",
            "|Michael  |Rose      |        |2000-05-19|M     |400000|\n",
            "|Robert   |          |Williams|1978-09-05|M     |400000|\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |400000|\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-100  |\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n",
            "+---------+----------+--------+----------+------+------+------------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|copiedColumn|\n",
            "+---------+----------+--------+----------+------+------+------------+\n",
            "|James    |          |Smith   |1991-04-01|M     |3000  |3000        |\n",
            "|Michael  |Rose      |        |2000-05-19|M     |4000  |4000        |\n",
            "|Robert   |          |Williams|1978-09-05|M     |4000  |4000        |\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |4000        |\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |-1          |\n",
            "+---------+----------+--------+----------+------+------+------------+\n",
            "\n",
            "+---------+----------+--------+----------+------+------+-------+-------------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|country|AnotherValue |\n",
            "+---------+----------+--------+----------+------+------+-------+-------------+\n",
            "|James    |          |Smith   |1991-04-01|M     |3000  |USA    |another_value|\n",
            "|Michael  |Rose      |        |2000-05-19|M     |4000  |USA    |another_value|\n",
            "|Robert   |          |Williams|1978-09-05|M     |4000  |USA    |another_value|\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |USA    |another_value|\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |USA    |another_value|\n",
            "+---------+----------+--------+----------+------+------+-------+-------------+\n",
            "\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark withColumnRenamed to Rename Column on DataFrame\n",
        "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
        "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
        "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
        "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
        "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "schema = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\n",
        "             StructField('middlename', StringType(), True),\n",
        "             StructField('lastname', StringType(), True)\n",
        "             ])),\n",
        "         StructField('dob', StringType(), True),\n",
        "         StructField('gender', StringType(), True),\n",
        "         StructField('salary', IntegerType(), True)\n",
        "         ])\n",
        "df = spark_session.createDataFrame(dataDF,schema)\n",
        "df.printSchema()\n",
        "\n",
        "#1. PySpark withColumnRenamed – To rename DataFrame column name\n",
        "df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n",
        "\n",
        "#2. PySpark withColumnRenamed – To rename multiple columns\n",
        "df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
        "  .withColumnRenamed(\"gender\",\"sex\").printSchema()\n",
        "\n",
        "#3. Using PySpark StructType – To rename a nested column in Dataframe\n",
        "schema2 = StructType([\n",
        "    StructField(\"fname\",StringType()),\n",
        "    StructField(\"middlename\",StringType()),\n",
        "    StructField(\"lname\",StringType())])\n",
        "\n",
        "df.select(col(\"name\").cast(schema2), \\\n",
        "          col(\"dob\"), col(\"gender\"), col(\"salary\")) \\\n",
        "  .printSchema()\n",
        "\n",
        "#4. Using Select – To rename nested elements.\n",
        "df.select(col(\"name.firstname\").alias(\"fname\"), \\\n",
        "  col(\"name.middlename\").alias(\"mname\"), \\\n",
        "  col(\"name.lastname\").alias(\"lname\"), \\\n",
        "  col(\"dob\"),col(\"gender\"),col(\"salary\")) \\\n",
        "  .printSchema()\n",
        "\n",
        "#5. Using PySpark DataFrame withColumn – To rename nested columns\n",
        "df.withColumn(\"fname\",col(\"name.firstname\")) \\\n",
        "  .withColumn(\"mname\",col(\"name.middlename\")) \\\n",
        "  .withColumn(\"lname\",col(\"name.lastname\")) \\\n",
        "  .drop(\"name\").printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj5DbZNaDUg6",
        "outputId": "a12ef4e8-4225-4014-ca4c-6d27a4277aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- DateOfBirth: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- DateOfBirth: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- fname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- fname: string (nullable = true)\n",
            " |-- mname: string (nullable = true)\n",
            " |-- lname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- fname: string (nullable = true)\n",
            " |-- mname: string (nullable = true)\n",
            " |-- lname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Using toDF() – To change all columns in a PySpark DataFrame\n",
        "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
        "df.toDF(*newColumns).printSchema()\n"
      ],
      "metadata": {
        "id": "nSRrSRpnH8JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark where() & filter() for efficient data filtering\n",
        "# Consider below example\n",
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField\n",
        "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
        "\n",
        "# Create SparkSession object\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Create data\n",
        "data = [\n",
        "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
        "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
        "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
        "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
        " ]\n",
        "\n",
        "# Create schema\n",
        "schema = StructType([\n",
        "     StructField('name', StructType([\n",
        "        StructField('firstname', StringType(), True),\n",
        "        StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "     ])),\n",
        "     StructField('languages', ArrayType(StringType()), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        " ])\n",
        "\n",
        "# Create dataframe\n",
        "df = spark.createDataFrame(data = data, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "# 1: DataFrame filter() with Column Condition\n",
        "df.filter(df.state == 'OH').show(truncate=False)\n",
        "\n",
        "#2: Using not equal filter condition\n",
        "df.filter(df.state != 'OH').show(truncate=False)\n",
        "df.filter(~(df.state == 'OH')).show(truncate=False)\n",
        "\n",
        "#3: Using col() function\n",
        "df.filter(col(\"state\") == 'OH').show(truncate=False)\n",
        "\n",
        "#4: Filtering with SQL Expression\n",
        "df.filter(\"gender == 'M'\").show(truncate=False)\n",
        "df.filter(\"gender != 'M'\").show(truncate=False)\n",
        "df.filter(\"gender <> 'M'\").show(truncate=False)\n",
        "\n",
        "#5. PySpark Filter with Multiple Conditions\n",
        "df.filter((df.state == 'OH') & (df.gender == 'M')).show(truncate=False)\n",
        "df.filter((col(\"state\") == 'OH') | (col(\"gender\") == 'M')).show(truncate=False)\n",
        "\n",
        "#6. Filter Based on List Values\n",
        "li=[\"OH\",\"CA\",\"DE\"]\n",
        "df.filter(df.state.isin(li)).show(truncate=False)\n",
        "\n",
        "#7. Filter Based on Starts With, Ends With, Contains\n",
        "df.filter(df.state.startswith(\"N\")).show(truncate=False)\n",
        "df.filter(df.state.endswith(\"O\")).show(truncate=False)\n",
        "df.filter(df.state.contains(\"O\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahRiuNTGJtc3",
        "outputId": "46938058-da20-4526-920c-98146f62c694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- languages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+-------------------+------------------+-----+------+\n",
            "|name               |languages         |state|gender|\n",
            "+-------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }     |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}|[CSharp, VB]      |OH   |F     |\n",
            "+-------------------+------------------+-----+------+\n",
            "\n",
            "+-------------------+------------------+-----+------+\n",
            "|name               |languages         |state|gender|\n",
            "+-------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }     |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}|[CSharp, VB]      |OH   |F     |\n",
            "+-------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+----+---------+-----+------+\n",
            "|name|languages|state|gender|\n",
            "+----+---------+-----+------+\n",
            "+----+---------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Filtering with Regular Expression\n",
        "# Prepare Data\n",
        "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n",
        "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
        "  ]\n",
        "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
        "\n",
        "df2.filter(col(\"name\").like(\"%rose%\")).show(truncate=False)\n",
        "\n",
        "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZeokSIoSmwi",
        "outputId": "dac9ee47-7554-47fc-9044-210c1ce826e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|id |name      |\n",
            "+---+----------+\n",
            "|5  |Rames rose|\n",
            "+---+----------+\n",
            "\n",
            "+---+------------+\n",
            "| id|        name|\n",
            "+---+------------+\n",
            "|  2|Michael Rose|\n",
            "|  4|  Rames Rose|\n",
            "|  5|  Rames rose|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering Array column\n",
        "from pyspark.sql.functions import array_contains\n",
        "df.filter(array_contains(df.languages,\"Java\")) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "# Filtering on Nested Struct columns\n",
        "df.filter(df.name.firstname == 'James') \\\n",
        "  .show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5OU-rSLUSyh",
        "outputId": "159b28c6-345a-416c-dce3-70c85bdf46e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------------+-----+------+\n",
            "|name            |languages         |state|gender|\n",
            "+----------------+------------------+-----+------+\n",
            "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
            "|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n",
            "+----------------+------------------+-----+------+\n",
            "\n",
            "+----------------+------------------+-----+------+\n",
            "|name            |languages         |state|gender|\n",
            "+----------------+------------------+-----+------+\n",
            "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
            "+----------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Distinct to Drop Duplicate Rows\n",
        "#Consider below example\n",
        "\n",
        "# Prepare Data\n",
        "data = [(\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600), \\\n",
        "    (\"Robert\", \"Sales\", 4100), \\\n",
        "    (\"Maria\", \"Finance\", 3000), \\\n",
        "    (\"James\", \"Sales\", 3000), \\\n",
        "    (\"Scott\", \"Finance\", 3300), \\\n",
        "    (\"Jen\", \"Finance\", 3900), \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000), \\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  ]\n",
        "\n",
        "# Create DataFrame\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "df.show(truncate=False)\n",
        "originalDFCount = df.count()\n",
        "print(f\"Original DF Count : {originalDFCount}\")\n",
        "#1:Get Distinct Rows (By Comparing All Columns)\n",
        "df.distinct().show(truncate=False)\n",
        "DistinctCount = df.distinct().count()\n",
        "print(f\"Removed Duplicates DF Count : {DistinctCount}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1_w1RZDZ6ID",
        "outputId": "b6c19867-cb9c-46d8-8865-6049a489b515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Original DF Count : 10\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Michael      |Sales     |4600  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Removed Duplicates DF Count : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Distinct of Selected Multiple Columns\n",
        "disDF = df.select(\"department\",\"salary\").distinct()\n",
        "disDF.show(truncate=False)\n",
        "\n",
        "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
        "dropDisDF.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzffRJhEdRrh",
        "outputId": "bb96be5b-50ab-4feb-f65a-920591409124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|department|salary|\n",
            "+----------+------+\n",
            "|Sales     |4600  |\n",
            "|Sales     |4100  |\n",
            "|Finance   |3000  |\n",
            "|Sales     |3000  |\n",
            "|Finance   |3900  |\n",
            "|Finance   |3300  |\n",
            "|Marketing |2000  |\n",
            "|Marketing |3000  |\n",
            "+----------+------+\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Maria        |Finance   |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Michael      |Sales     |4600  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark orderBy() and sort() explained\n",
        "#Consider below example\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df = spark_session.createDataFrame(simpleData,columns)\n",
        "df.printSchema()\n",
        "\n",
        "#1.DataFrame sorting using the sort() function\n",
        "#df.sort(\"column1\", \"column2\", ascending=[True, False])\n",
        "from pyspark.sql.functions import col\n",
        "df.sort(\"department\",\"state\").show(truncate=False)\n",
        "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
        "\n",
        "#2.DataFrame sorting using orderBy() function\n",
        "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
        "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
        "\n",
        "#3.Sort by Ascending (ASC)\n",
        "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
        "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
        "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
        "\n",
        "#4.Sort by Descending (DESC)\n",
        "df.sort(df.department.desc(),df.state.desc()).show(truncate=False)\n",
        "df.sort(col(\"department\").desc(),col(\"state\").desc()).show(truncate=False)\n",
        "\n",
        "#5. Using Raw SQL\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark_session.sql(\"select * from emp order by department desc\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGaRqTa1fMLg",
        "outputId": "6090ddec-800c-4d75-cd35-ae5844d723ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Groupby Explained with Example\n",
        "#consider below example\n",
        "\n",
        "# Data\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n",
        "\n",
        "# Create DataFrame\n",
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df = spark_session.createDataFrame(simpleData,schema)\n",
        "\n",
        "#1. PySpark groupBy on DataFrame Columns\n",
        "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
        "\n",
        "#2. Using Multiple columns\n",
        "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show(truncate=False)\n",
        "\n",
        "#3. Running more aggregates at a time\n",
        "from pyspark.sql.functions import sum,avg,max\n",
        "df.groupBy(\"department\") \\\n",
        "  .agg(sum(\"salary\").alias(\"Total Salary\"), \\\n",
        "       avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "       sum(\"bonus\").alias(\"Total Bonus\"), \\\n",
        "       max(\"bonus\").alias(\"Maximum Bonus\") \\\n",
        "       ) \\\n",
        "       .show(truncate=False)\n",
        "\n",
        "#4. Using filter on aggregate data\n",
        "df.groupBy(\"department\") \\\n",
        "  .agg(sum(\"salary\").alias(\"Total Salary\"), \\\n",
        "       avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "       sum(\"bonus\").alias(\"Total Bonus\"), \\\n",
        "       max(\"bonus\").alias(\"Maximum Bonus\") \\\n",
        "       ) \\\n",
        "  .where(col(\"Total Bonus\") >= 50000) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "#5. PySpark SQL GROUP BY Query\n",
        "df.createOrReplaceTempView(\"emp\")\n",
        "sql_string = \"\"\"SELECT department,\n",
        "       SUM(salary) AS sum_salary,\n",
        "       AVG(salary) AS avg_salary,\n",
        "       SUM(bonus) AS sum_bonus,\n",
        "       MAX(bonus) AS max_bonus\n",
        "FROM emp\n",
        "GROUP BY department\n",
        "HAVING SUM(bonus) >= 50000\"\"\"\n",
        "\n",
        "\n",
        "spark_session.sql(sql_string).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5MZxOhsKc-q",
        "outputId": "0ca3dec9-4114-4064-f8b3-7d3ffc4c9f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|department|sum(salary)|\n",
            "+----------+-----------+\n",
            "|Sales     |257000     |\n",
            "|Finance   |351000     |\n",
            "|Marketing |171000     |\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----+-----------+----------+\n",
            "|department|state|sum(salary)|sum(bonus)|\n",
            "+----------+-----+-----------+----------+\n",
            "|Sales     |CA   |81000      |23000     |\n",
            "|Finance   |CA   |189000     |47000     |\n",
            "|Sales     |NY   |176000     |30000     |\n",
            "|Finance   |NY   |162000     |34000     |\n",
            "|Marketing |NY   |91000      |21000     |\n",
            "|Marketing |CA   |80000      |18000     |\n",
            "+----------+-----+-----------+----------+\n",
            "\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|department|Total Salary|avg_salary       |Total Bonus|Maximum Bonus|\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|Sales     |257000      |85666.66666666667|53000      |23000        |\n",
            "|Finance   |351000      |87750.0          |81000      |24000        |\n",
            "|Marketing |171000      |85500.0          |39000      |21000        |\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|department|Total Salary|avg_salary       |Total Bonus|Maximum Bonus|\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|Sales     |257000      |85666.66666666667|53000      |23000        |\n",
            "|Finance   |351000      |87750.0          |81000      |24000        |\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
            "|Finance   |351000    |87750.0          |81000    |24000    |\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Join Types | Join Two DataFrames\n",
        "'''\n",
        "Inner Join: Returns only the rows with matching keys in both DataFrames.\n",
        "Left Join: Returns all rows from the left DataFrame and matching rows from the right DataFrame.\n",
        "Right Join: Returns all rows from the right DataFrame and matching rows from the left DataFrame.\n",
        "Full Outer Join: Returns all rows from both DataFrames, including matching and non-matching rows.\n",
        "Left Semi Join: Returns all rows from the left DataFrame where there is a match in the right DataFrame.\n",
        "Left Anti Join: Returns all rows from the left DataFrame where there is no match in the right DataFrame.\n",
        "'''\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "       (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "       (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "       (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "       (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "       (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "]\n",
        "\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "empDF = spark_session.createDataFrame(emp,empColumns)\n",
        "deptDF = spark_session.createDataFrame(dept,deptColumns)\n",
        "\n",
        "#1: Inner Join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"inner\").show(truncate=False)\n",
        "\n",
        "#2: Left Join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"left\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"leftouter\").show(truncate=False)\n",
        "\n",
        "#3: Righ Join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"right\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"rightouter\").show(truncate=False)\n",
        "\n",
        "#4: Full outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"full\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"fullouter\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"outer\").show(truncate=False)\n",
        "\n",
        "#5: Left Semi join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"leftsemi\").show(truncate=False)\n",
        "\n",
        "#6: Left Anti join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show(truncate=False)\n",
        "\n",
        "#7: Self join\n",
        "from pyspark.sql.functions import col\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "id": "wJ3O1ix4RfWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebdd171-4755-4b80-fb7f-4af9ebe97cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|6     |Brown|2              |2010       |50         |      |-1    |\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "\n",
            "+------+--------+---------------+-----------------+\n",
            "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
            "+------+--------+---------------+-----------------+\n",
            "|2     |Rose    |1              |Smith            |\n",
            "|3     |Williams|1              |Smith            |\n",
            "|4     |Jones   |2              |Rose             |\n",
            "|5     |Brown   |2              |Rose             |\n",
            "|6     |Brown   |2              |Rose             |\n",
            "+------+--------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SQL Expression\n",
        "empDF.createOrReplaceTempView(\"emp\")\n",
        "deptDF.createOrReplaceTempView(\"dept\")\n",
        "\n",
        "spark_session.sql(\"select * from emp e, dept d where e.emp_dept_id == d.dept_id\").show(truncate=False)\n",
        "spark_session.sql(\"select * from emp e inner join dept d on e.emp_dept_id == d.dept_id\").show(truncate=False)\n",
        "\n",
        "# PySpark SQL Join on multiple DataFrames\n",
        "# Join on multiple dataFrames\n",
        "df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
        "   .join(df3,df1.id1 == df3.id3,\"inner\")"
      ],
      "metadata": {
        "id": "ZEwyUtNO8mKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Union and UnionAll Explained\n",
        "# union and unionAll - it will merge two dataframes with duplicates\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
        "  ]\n",
        "\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df1 = spark_session.createDataFrame(simpleData,columns)\n",
        "df2 = spark_session.createDataFrame(simpleData2,columns2)\n",
        "\n",
        "unionDF = df1.union(df2)\n",
        "unionDF.show(truncate=False)\n",
        "\n",
        "unionAllDF = df1.unionAll(df2)\n",
        "unionAllDF.show(truncate=False)\n",
        "\n",
        "# Merge without Duplicates\n",
        "\n",
        "MergeDistinctDF = df1.union(df2).distinct()\n",
        "MergeDistinctDF.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4iGVWEyTZL6",
        "outputId": "62f1acea-ab98-459d-a2a4-dcd563c096cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark unionByName()\n",
        "#unionByName(df, allowMissingColumns=True)\n",
        "\n",
        "data1 = [(\"James\",34), (\"Michael\",56), \\\n",
        "        (\"Robert\",30), (\"Maria\",24) ]\n",
        "\n",
        "data2 = [(34,\"James\"),(45,\"Maria\"), \\\n",
        "        (45,\"Jen\"),(34,\"Jeff\")]\n",
        "\n",
        "df1 = spark_session.createDataFrame(data1,[\"name\",\"id\"])\n",
        "df2 = spark_session.createDataFrame(data2,[\"id\",\"name\"])\n",
        "\n",
        "df1.unionByName(df2).show(truncate=False)\n",
        "\n",
        "# Use unionByName() with Different Number of Columns\n",
        "# Create DataFrames with different column names\n",
        "df1 = spark_session.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark_session.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# Using allowMissingColumns\n",
        "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
        "df3.printSchema\n",
        "df3.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Odzw5O0fNHB",
        "outputId": "db108cbf-473e-42ff-a41c-53af1f0434a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|name   |id |\n",
            "+-------+---+\n",
            "|James  |34 |\n",
            "|Michael|56 |\n",
            "|Robert |30 |\n",
            "|Maria  |24 |\n",
            "|James  |34 |\n",
            "|Maria  |45 |\n",
            "|Jen    |45 |\n",
            "|Jeff   |34 |\n",
            "+-------+---+\n",
            "\n",
            "+----+----+----+----+\n",
            "|col0|col1|col2|col3|\n",
            "+----+----+----+----+\n",
            "|   5|   2|   6|NULL|\n",
            "|NULL|   6|   7|   3|\n",
            "+----+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark UDF (User Defined Function)\n",
        "# Consider below example\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "        (\"2\", \"tracey smith\"),\n",
        "        (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#Create a Python Function\n",
        "def CamelCase(str):\n",
        "  resstr = ''\n",
        "  arr = str.split(\" \")\n",
        "  for x in arr:\n",
        "    resstr = resstr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "  return resstr\n",
        "\n",
        "#Convert a Python function to PySpark UDF\n",
        "from pyspark.sql.functions import udf,StringType,col\n",
        "convertUDF = udf(lambda z: CamelCase(z),StringType())\n",
        "\n",
        "df.select(col(\"Seqno\"),convertUDF(col(\"Name\")).alias(\"Name\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h39F5h9gbjQ",
        "outputId": "606b2efd-c909-4204-d417-38041fa7544e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|Seqno|Name         |\n",
            "+-----+-------------+\n",
            "|1    |John Jones   |\n",
            "|2    |Tracey Smith |\n",
            "|3    |Amy Sanders  |\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Registering PySpark UDF & use it on SQL\n",
        "spark_session.udf.register(\"convertUDF\",CamelCase,StringType())\n",
        "df.createOrReplaceTempView(\"Test\")\n",
        "spark_session.sql(\"select Seqno, convertUDF(name) AS \"\"Name\"\" from Test\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8U8MiZIl5dA",
        "outputId": "f36ecf12-3048-4d6b-a60d-4ea5151a5588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|Seqno|         Name|\n",
            "+-----+-------------+\n",
            "|    1|  John Jones |\n",
            "|    2|Tracey Smith |\n",
            "|    3| Amy Sanders |\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating UDF using annotation\n",
        "@udf(returnType=StringType())\n",
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n",
        ".show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A06pM72cnNXY",
        "outputId": "b17df662-d084-43f4-9bc4-1b93b17a5100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+-------------+\n",
            "|Seqno|Name        |Cureated Name|\n",
            "+-----+------------+-------------+\n",
            "|1    |john jones  |JOHN JONES   |\n",
            "|2    |tracey smith|TRACEY SMITH |\n",
            "|3    |amy sanders |AMY SANDERS  |\n",
            "+-----+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark map() Transformation\n",
        "# map()in PySpark is a transformation function that is used to apply a function/lambda to each element of an RDD (Resilient Distributed Dataset)\n",
        "\n",
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd = spark_session.sparkContext.parallelize(data)\n",
        "\n",
        "rdd2 = rdd.map(lambda x: (x,1))\n",
        "for element in rdd2.collect():\n",
        "  print(element)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92IFsyBNpKJG",
        "outputId": "59eefebe-5997-4e78-8ace-cc3044dadfe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Project', 1)\n",
            "('Gutenberg’s', 1)\n",
            "('Alice’s', 1)\n",
            "('Adventures', 1)\n",
            "('in', 1)\n",
            "('Wonderland', 1)\n",
            "('Project', 1)\n",
            "('Gutenberg’s', 1)\n",
            "('Adventures', 1)\n",
            "('in', 1)\n",
            "('Wonderland', 1)\n",
            "('Project', 1)\n",
            "('Gutenberg’s', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark map() Example with DataFrame\n",
        "data = [('James','Smith','M',30),\n",
        "  ('Anna','Rose','F',41),\n",
        "  ('Robert','Williams','M',62),\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
        "df = spark_session.createDataFrame(data=data, schema = columns)\n",
        "\n",
        "# Refering columns by index.\n",
        "rdd2 = df.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*2))\n",
        "\n",
        "df2 = rdd2.toDF([\"name\",\"gender\",\"new_salary\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdyHDIh1sUxg",
        "outputId": "a9c7ed38-1251-4d68-d83e-1882213c0800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------+----------+\n",
            "|           name|gender|new_salary|\n",
            "+---------------+------+----------+\n",
            "|    James,Smith|     M|        60|\n",
            "|      Anna,Rose|     F|        82|\n",
            "|Robert,Williams|     M|       124|\n",
            "+---------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark flatMap() Transformation\n",
        "#PySpark flatMap() is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame.\n",
        "#Consider below example\n",
        "\n",
        "data = [\"Project Gutenberg’s\",\n",
        "        \"Alice’s Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\",\n",
        "        \"Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\"]\n",
        "rdd=spark_session.sparkContext.parallelize(data)\n",
        "for element in rdd.collect():\n",
        "    print(element)\n",
        "\n",
        "\n",
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzqEs-rLuL2A",
        "outputId": "9fe30fbd-c7c9-4170-e9fd-a72e12c87c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Gutenberg’s\n",
            "Alice’s Adventures in Wonderland\n",
            "Project Gutenberg’s\n",
            "Adventures in Wonderland\n",
            "Project Gutenberg’s\n",
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark foreach() Usage with Examples\n",
        "# This is different than other actions as foreach() function doesn’t return a value instead it executes the input function on each element of an RDD, DataFrame\n",
        "\n",
        "#Consider below example\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "        (\"2\", \"tracey smith\"),\n",
        "        (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "# foreach() Example\n",
        "def f(df):\n",
        "    print(df.Seqno)\n",
        "\n",
        "df.foreach(f)\n"
      ],
      "metadata": {
        "id": "RNcJ850OyxbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample,sampleBy,transform,apply pending to learn"
      ],
      "metadata": {
        "id": "ILcFl3qw1d_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark fillna() & fill() – Replace NULL/None Values\n",
        "filepath = '/content/small_zipcode.csv'\n",
        "df = spark_session.read.options(header='true',inferSchema='true').csv(filepath)\n",
        "\n",
        "#PySpark Replace NULL/None Values with Zero (0)\n",
        "\n",
        "#Replace 0 for null for all integer columns\n",
        "df.na.fill(value=0).show()\n",
        "\n",
        "#We want to define specific columns\n",
        "df.na.fill(value=0,subset=[\"population\"]).show()\n",
        "\n",
        "#PySpark Replace Null/None Value with Empty String\n",
        "df.na.fill(\"\").show()\n",
        "\n",
        "df.na.fill(\"UNKNOWN\",[\"city\"]) \\\n",
        "  .na.fill(\"\",[\"type\"]).show()\n",
        "\n",
        "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRRVgOtX1vzE",
        "outputId": "70bbd2fc-e211-49ab-bbe8-6edb11851011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
            "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
            "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
            "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
            "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|                   |   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|                   |   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|            UNKNOWN|   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|            UNKNOWN|   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|            unknown|   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Pivot and Unpivot DataFrame\n",
        "# pivot_df = original_df.groupBy(\"grouping_column\").pivot(\"pivot_column\").agg({\"agg_column\": \"agg_function\"})\n",
        "#grouping_column: The column used for grouping.\n",
        "#pivot_column: The column whose distinct values become new columns.\n",
        "#agg_column: The column for which aggregation is applied (e.g., using a function like sum, avg, etc.).\n",
        "\n",
        "#Consider below example\n",
        "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
        "        (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
        "        (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
        "        (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
        "\n",
        "columns= [\"Product\",\"Amount\",\"Country\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "pivotDF = df.groupBy(\"Product\").pivot(\"country\").sum(\"Amount\")\n",
        "pivotDF.show()\n",
        "\n",
        "#Unpivot Dataframe\n",
        "#Unpivot is a reverse operation, we can achieve by rotating column values into rows values.\n",
        "from pyspark.sql.functions import expr\n",
        "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
        "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
        "    .where(\"Total is not null\")\n",
        "unPivotDF.show(truncate=False)\n",
        "unPivotDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK01HYxJ6vjh",
        "outputId": "ab198931-42fe-45b5-f559-33cab10d175f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----+------+----+\n",
            "|Product|Canada|China|Mexico| USA|\n",
            "+-------+------+-----+------+----+\n",
            "| Orange|  NULL| 4000|  NULL|4000|\n",
            "|  Beans|  NULL| 1500|  2000|1600|\n",
            "| Banana|  2000|  400|  NULL|1000|\n",
            "|Carrots|  2000| 1200|  NULL|1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n",
            "+-------+-------+-----+\n",
            "|Product|Country|Total|\n",
            "+-------+-------+-----+\n",
            "|Orange |China  |4000 |\n",
            "|Beans  |China  |1500 |\n",
            "|Beans  |Mexico |2000 |\n",
            "|Banana |Canada |2000 |\n",
            "|Banana |China  |400  |\n",
            "|Carrots|Canada |2000 |\n",
            "|Carrots|China  |1200 |\n",
            "+-------+-------+-----+\n",
            "\n",
            "+-------+-------+-----+\n",
            "|Product|Country|Total|\n",
            "+-------+-------+-----+\n",
            "| Orange|  China| 4000|\n",
            "|  Beans|  China| 1500|\n",
            "|  Beans| Mexico| 2000|\n",
            "| Banana| Canada| 2000|\n",
            "| Banana|  China|  400|\n",
            "|Carrots| Canada| 2000|\n",
            "|Carrots|  China| 1200|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark MapType (Dict) Usage with Examples\n",
        "# PySpark MapType is used to represent map key-value pair similar to python Dictionary (Dict),\n",
        "\n",
        "#1:Create PySpark MapType\n",
        "from pyspark.sql.types import StringType, MapType\n",
        "mapCol = MapType(StringType(),StringType(),False)\n",
        "\n",
        "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
        "schema = StructType([\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('properties', MapType(StringType(),StringType()),True)\n",
        "])\n",
        "from pyspark.sql import SparkSession\n",
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'brown','eye':''})\n",
        "        ]\n",
        "df = spark_session.createDataFrame(data=dataDictionary, schema = schema)\n",
        "df.printSchema()\n",
        "\n",
        "df3 = df.rdd.map(lambda x: (x.name,x.properties[\"hair\"],x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\n",
        "df3.show(truncate=False)"
      ],
      "metadata": {
        "id": "McSBnMqVAKxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b729c6e-2538-4fbe-a254-ec6b829070cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-----+-----+\n",
            "|name      |hair |eye  |\n",
            "+----------+-----+-----+\n",
            "|James     |black|brown|\n",
            "|Michael   |brown|NULL |\n",
            "|Robert    |red  |black|\n",
            "|Washington|grey |grey |\n",
            "|Jefferson |brown|     |\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Aggregate Functions\n",
        "# Consider below example\n",
        "\n",
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(simpleData,schema)\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "#1:approx_count_distinct Aggregate Function\n",
        "# returns the count of distinct items in a group.\n",
        "print(\"approx_count_distinct: \" + \\\n",
        "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
        "\n",
        "#2:average() aggregate function- returns the average of values in the input col\n",
        "print(\"avg : \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
        "\n",
        "#3:collect_list Aggregate Function\n",
        "# returns all values from an input column with duplicates.\n",
        "df.select(collect_list(\"salary\")).show(truncate=False)\n",
        "\n",
        "#4:collect_set() Aggregate Function\n",
        "#all values from an input column with duplicate values eliminated.\n",
        "df.select(collect_set(\"salary\")).show(truncate=False)\n",
        "\n",
        "#5:countDistinct Aggregate Function\n",
        "#returns the number of distinct elements in a columns\n",
        "df2 = df.select(countDistinct(\"department\",\"salary\"))\n",
        "print(\"Distinct count of Department and salary: \"+str(df2.collect()[0][0]))\n",
        "\n",
        "#6:count function- returns number of elements in a column.\n",
        "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
        "\n",
        "#7:first function\n",
        "df.select(first(\"salary\")).show(truncate=False)\n",
        "\n",
        "#8:last function\n",
        "df.select(last(\"salary\")).show(truncate=False)\n",
        "\n",
        "#9:kurtosis function\n",
        "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
        "\n",
        "#10:max function\n",
        "df.select(max(\"salary\")).show(truncate=False)\n",
        "\n",
        "#11:min function\n",
        "df.select(min(\"salary\")).show(truncate=False)\n",
        "\n",
        "#12:mean function\n",
        "df.select(mean(\"salary\")).show(truncate=False)\n",
        "\n",
        "#13:skewness function\n",
        "df.select(skewness(\"salary\")).show(truncate=False)\n",
        "\n",
        "#14:stddev(), stddev_samp() and stddev_pop()\n",
        "df.select(stddev(\"salary\"),stddev_samp(\"salary\"),stddev_pop(\"salary\")).show(truncate=False)\n",
        "\n",
        "#15:sumDistinct function\n",
        "df.select(sumDistinct(\"salary\").alias(\"Sum_Distinct\")).show(truncate=False)\n",
        "\n",
        "#16:variance(), var_samp(), var_pop()\n",
        "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")).show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ZBip8OHLdV",
        "outputId": "ab37ec6c-f0c5-4bac-e90d-642207ad93a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "approx_count_distinct: 6\n",
            "avg : 3400.0\n",
            "+------------------------------------------------------------+\n",
            "|collect_list(salary)                                        |\n",
            "+------------------------------------------------------------+\n",
            "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
            "+------------------------------------------------------------+\n",
            "\n",
            "+------------------------------------+\n",
            "|collect_set(salary)                 |\n",
            "+------------------------------------+\n",
            "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
            "+------------------------------------+\n",
            "\n",
            "Distinct count of Department and salary: 8\n",
            "count: Row(count(salary)=10)\n",
            "+-------------+\n",
            "|first(salary)|\n",
            "+-------------+\n",
            "|3000         |\n",
            "+-------------+\n",
            "\n",
            "+------------+\n",
            "|last(salary)|\n",
            "+------------+\n",
            "|4100        |\n",
            "+------------+\n",
            "\n",
            "+-------------------+\n",
            "|kurtosis(salary)   |\n",
            "+-------------------+\n",
            "|-0.6467803030303032|\n",
            "+-------------------+\n",
            "\n",
            "+-----------+\n",
            "|max(salary)|\n",
            "+-----------+\n",
            "|4600       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|min(salary)|\n",
            "+-----------+\n",
            "|2000       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|avg(salary)|\n",
            "+-----------+\n",
            "|3400.0     |\n",
            "+-----------+\n",
            "\n",
            "+--------------------+\n",
            "|skewness(salary)    |\n",
            "+--------------------+\n",
            "|-0.12041791181069571|\n",
            "+--------------------+\n",
            "\n",
            "+-----------------+-------------------+------------------+\n",
            "|stddev(salary)   |stddev_samp(salary)|stddev_pop(salary)|\n",
            "+-----------------+-------------------+------------------+\n",
            "|765.9416862050705|765.9416862050705  |726.636084983398  |\n",
            "+-----------------+-------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
            "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|Sum_Distinct|\n",
            "+------------+\n",
            "|20900       |\n",
            "+------------+\n",
            "\n",
            "+-----------------+-----------------+---------------+\n",
            "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
            "+-----------------+-----------------+---------------+\n",
            "|586666.6666666666|586666.6666666666|528000.0       |\n",
            "+-----------------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Window Functions\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark_session.createDataFrame(data = simpleData, schema = columns)\n",
        "WindowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "#2.1:row_number() window function - gives the sequential row number starting from 1 to the result of each window partition.\n",
        "df.withColumn(\"row_number\",row_number().over(WindowSpec)).show(truncate=False)\n",
        "\n",
        "#2.2:rank Window Function - provides a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
        "df.withColumn(\"rank\",rank().over(WindowSpec)).show(truncate=False)\n",
        "\n",
        "#2.3:dense_rank Window Function -  used to get the result with rank of rows within a window partition without any gaps.\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(WindowSpec)).show(truncate=False)\n",
        "\n",
        "#2.4:percent_rank Window Function\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(WindowSpec)).show(truncate=False)\n",
        "\n",
        "#2.5 ntile Window Function\n",
        "#returns the relative rank of result rows within a window partition.\n",
        "df.withColumn(\"ntile\",ntile(2).over(WindowSpec)).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "2eSIrA1J1CaC",
        "outputId": "d12a35cb-b077-44f0-ce8a-aabfd615b299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|James        |Sales     |3000  |2         |\n",
            "|Robert       |Sales     |4100  |3         |\n",
            "|Saif         |Sales     |4100  |4         |\n",
            "|Michael      |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rank' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4039878049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#2.2:rank Window Function - provides a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindowSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#2.3:dense_rank Window Function -  used to get the result with rank of rows within a window partition without any gaps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rank' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Window Analytic Functions\n",
        "#3.1 cume_dist Window Function:\n",
        "# cume_dist() Example\n",
        "from pyspark.sql.functions import cume_dist\n",
        "df.withColumn(\"cume_dist\",cume_dist().over(WindowSpec)).show(truncate=False)\n",
        "\n",
        "#3.2 lag Window Function\n",
        "#The lag() function allows you to access a previous row’s value within the partition based on a specified offset.\n",
        "df.withColumn(\"lag\",lag(\"salary\",2).over(WindowSpec)).show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ywTHfeRAbxg",
        "outputId": "7ba81e78-1027-4391-90be-9fe468a03987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|cume_dist         |\n",
            "+-------------+----------+------+------------------+\n",
            "|Maria        |Finance   |3000  |0.3333333333333333|\n",
            "|Scott        |Finance   |3300  |0.6666666666666666|\n",
            "|Jen          |Finance   |3900  |1.0               |\n",
            "|Kumar        |Marketing |2000  |0.5               |\n",
            "|Jeff         |Marketing |3000  |1.0               |\n",
            "|James        |Sales     |3000  |0.4               |\n",
            "|James        |Sales     |3000  |0.4               |\n",
            "|Robert       |Sales     |4100  |0.8               |\n",
            "|Saif         |Sales     |4100  |0.8               |\n",
            "|Michael      |Sales     |4600  |1.0               |\n",
            "+-------------+----------+------+------------------+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lag |\n",
            "+-------------+----------+------+----+\n",
            "|Maria        |Finance   |3000  |NULL|\n",
            "|Scott        |Finance   |3300  |NULL|\n",
            "|Jen          |Finance   |3900  |3000|\n",
            "|Kumar        |Marketing |2000  |NULL|\n",
            "|Jeff         |Marketing |3000  |NULL|\n",
            "|James        |Sales     |3000  |NULL|\n",
            "|James        |Sales     |3000  |NULL|\n",
            "|Robert       |Sales     |4100  |3000|\n",
            "|Saif         |Sales     |4100  |3000|\n",
            "|Michael      |Sales     |4600  |4100|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.4 lead Window Function\n",
        "#the lead() function retrieves the column value from the following row within the partition based on a specified offset.\n",
        "from pyspark.sql.functions import lead\n",
        "df.withColumn(\"lead\",lead(\"salary\",2).over(WindowSpec)).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WdCyKdrM4I6",
        "outputId": "e5cd7b5c-07a7-4541-dab5-a4015b852aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lead|\n",
            "+-------------+----------+------+----+\n",
            "|Maria        |Finance   |3000  |3900|\n",
            "|Scott        |Finance   |3300  |NULL|\n",
            "|Jen          |Finance   |3900  |NULL|\n",
            "|Kumar        |Marketing |2000  |NULL|\n",
            "|Jeff         |Marketing |3000  |NULL|\n",
            "|James        |Sales     |3000  |4100|\n",
            "|James        |Sales     |3000  |4100|\n",
            "|Robert       |Sales     |4100  |4600|\n",
            "|Saif         |Sales     |4100  |NULL|\n",
            "|Michael      |Sales     |4600  |NULL|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Window Aggregate Functions\n",
        "# Aggregate functions examples\n",
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
        "df.withColumn(\"row\",row_number().over(WindowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "nPbaHPFKNaqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f26e0c8-8b19-442e-e5ec-d6820c335178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark SQL Date and Timestamp Functions - I will learn it in night"
      ],
      "metadata": {
        "id": "EyqGPQRJgH4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark JSON Functions with Examples\n",
        "'''\n",
        "1.from_json() - Converts JSON string into Struct type or Map type.\n",
        "2.to_json() - Converts MapType or Struct type to JSON string.\n",
        "3.json_tuple() - Extract the Data from JSON and create them as a new columns.\n",
        "4.get_json_object() - Extracts JSON element from a JSON string based on json path specified.\n",
        "5.schema_of_json() - Create schema string from JSON string\n",
        "'''\n",
        "#Consider below example\n",
        "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
        "df = spark_session.createDataFrame([(1,jsonString)],[\"id\",\"value\"])\n",
        "df.show(truncate=False)\n",
        "\n",
        "#2.1:from_json()- used to convert JSON string into Struct type or Map type.\n",
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import StringType,MapType\n",
        "df2 = df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27c6hEQLgj1B",
        "outputId": "987fe26f-00e1-4635-bb9f-4568c4f35af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------------------------------------------------+\n",
            "|id |value                                                                     |\n",
            "+---+--------------------------------------------------------------------------+\n",
            "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
            "+---+--------------------------------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- value: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+---+---------------------------------------------------------------------------+\n",
            "|id |value                                                                      |\n",
            "+---+---------------------------------------------------------------------------+\n",
            "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
            "+---+---------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2. to_json()-  used to convert DataFrame columns MapType or Struct type to JSON string.\n",
        "from pyspark.sql.functions import to_json\n",
        "df2.withColumn(\"value\",to_json(col(\"value\"))).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7v74XNejv-t",
        "outputId": "bafbdf98-b3db-4e85-84c4-399600c5559e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------------------------------------------------+\n",
            "|id |value                                                                       |\n",
            "+---+----------------------------------------------------------------------------+\n",
            "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
            "+---+----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.3. json_tuple() - used the query or extract the elements from JSON column and create the result as a new columns.\n",
        "from pyspark.sql.functions import json_tuple\n",
        "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\",\"State\"))\\\n",
        "  .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\",\"state\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bXERzEQkTCk",
        "outputId": "f05a9195-bca8-449d-a1c5-1a246922f621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------+-----------+-----+\n",
            "|id |Zipcode|ZipCodeType|City       |state|\n",
            "+---+-------+-----------+-----------+-----+\n",
            "|1  |704    |STANDARD   |PARC PARQUE|PR   |\n",
            "+---+-------+-----------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4. get_json_object() - used to extract the JSON string based on path from the JSON column.\n",
        "from pyspark.sql.functions import get_json_object\n",
        "df.select(col(\"id\"),get_json_object(col(\"value\"), \"$.ZipCodeType\").alias(\"ZipCodeType\")).show(truncate=False)\n",
        "\n",
        "#2.5. schema_of_json() - Use schema_of_json() to create schema string from JSON string column.\n",
        "\n",
        "from pyspark.sql.functions import schema_of_json,lit\n",
        "schemaStr=spark_session.range(1) \\\n",
        "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
        "    .collect()[0][0]\n",
        "print(schemaStr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7nqLU92pYsY",
        "outputId": "9605c821-fc6d-4f6b-d841-cecb178d4788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+\n",
            "|id |ZipCodeType|\n",
            "+---+-----------+\n",
            "|1  |STANDARD   |\n",
            "+---+-----------+\n",
            "\n",
            "STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Datasources- I will do it in the night"
      ],
      "metadata": {
        "id": "hthhQUaYr6Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark When Otherwise | SQL Case When Usage\n",
        "from pyspark.sql.functions import when\n",
        "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
        "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
        "        (\"Jen\",\"\",None)]\n",
        "\n",
        "columns = [\"name\",\"gender\",\"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#1. Using when() otherwise() on PySpark DataFrame.\n",
        "df.withColumn(\"new_gender\",when(df.gender == 'M',\"Male\")\n",
        "                          .when(df.gender == 'F',\"Female\")\n",
        "                          .when(df.gender.isNull(),\"\")\n",
        "                          .otherwise(df.gender)).show(truncate=False)\n",
        "\n",
        "#2. Using Case When Else on DataFrame using withColumn() & select()\n",
        "from pyspark.sql.functions import expr, col\n",
        "df.withColumn(\"new_gender\",expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
        "                                \"WHEN gender = 'F' THEN 'Female' \" +\n",
        "                                \"ELSE gender END\")).show(truncate=False)\n",
        "\n",
        "#3. #Using Case When on select()\n",
        "df.select(col(\"*\"),expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
        "                                \"WHEN gender = 'F' THEN 'Female' \" +\n",
        "                                \"ELSE gender END\").alias(\"new_gender\")).show(truncate=False)\n",
        "\n",
        "#4. Using Case When on SQL Expression\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark_session.sql(\"select name , CASE WHEN gender = 'M' THEN 'Male' \"+\n",
        "          \"when gender = 'F' THEN 'Female' \" +\n",
        "          \"when gender IS NULL THEN '' \" +\n",
        "          \"ELSE gender END as new_gender from EMP\").show(truncate=False)"
      ],
      "metadata": {
        "id": "bwykVbdHr9UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554f95bc-93c4-4aa8-dea6-a0653fa1013f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+----------+\n",
            "|name   |gender|salary|new_gender|\n",
            "+-------+------+------+----------+\n",
            "|James  |M     |60000 |Male      |\n",
            "|Michael|M     |70000 |Male      |\n",
            "|Robert |NULL  |400000|          |\n",
            "|Maria  |F     |500000|Female    |\n",
            "|Jen    |      |NULL  |          |\n",
            "+-------+------+------+----------+\n",
            "\n",
            "+-------+------+------+----------+\n",
            "|name   |gender|salary|new_gender|\n",
            "+-------+------+------+----------+\n",
            "|James  |M     |60000 |Male      |\n",
            "|Michael|M     |70000 |Male      |\n",
            "|Robert |NULL  |400000|NULL      |\n",
            "|Maria  |F     |500000|Female    |\n",
            "|Jen    |      |NULL  |          |\n",
            "+-------+------+------+----------+\n",
            "\n",
            "+-------+------+------+----------+\n",
            "|name   |gender|salary|new_gender|\n",
            "+-------+------+------+----------+\n",
            "|James  |M     |60000 |Male      |\n",
            "|Michael|M     |70000 |Male      |\n",
            "|Robert |NULL  |400000|NULL      |\n",
            "|Maria  |F     |500000|Female    |\n",
            "|Jen    |      |NULL  |          |\n",
            "+-------+------+------+----------+\n",
            "\n",
            "+-------+----------+\n",
            "|name   |new_gender|\n",
            "+-------+----------+\n",
            "|James  |Male      |\n",
            "|Michael|Male      |\n",
            "|Robert |          |\n",
            "|Maria  |Female    |\n",
            "|Jen    |          |\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple Conditions using & and | operator\n",
        "df5.withColumn(“new_column”, when((col(“code”) == “a”) | (col(“code”) == “d”), “A”)\n",
        ".when((col(“code”) == “b”) & (col(“amt”) == “4”), “B”)\n",
        ".otherwise(“A1”)).show()"
      ],
      "metadata": {
        "id": "loSbvPk2hkb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark SQL expr() (Expression) Function\n",
        "#1: Concatenate Columns using || (similar to SQL)\n",
        "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")]\n",
        "df = spark_session.createDataFrame(data,[\"col1\",\"col2\"])\n",
        "df.withColumn(\"Name\",expr(\"col1 ||',' ||col2\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1HvKArsmsDt",
        "outputId": "38b92ad4-1193-482a-915a-d8ce6235b833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+\n",
            "|col1 |col2 |Name       |\n",
            "+-----+-----+-----------+\n",
            "|James|Bond |James,Bond |\n",
            "|Scott|Varsa|Scott,Varsa|\n",
            "+-----+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2: Using SQL CASE WHEN with expr()\n",
        "from pyspark.sql.functions import expr\n",
        "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
        "columns = [\"name\",\"gender\"]\n",
        "df = spark_session.createDataFrame(data = data, schema = columns)\n",
        "\n",
        "#Using CASE WHEN similar to SQL.\n",
        "from pyspark.sql.functions import expr\n",
        "df2=df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
        "           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "6zl6VO2Cpp70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3: Using an Existing Column Value for Expression\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
        "df=spark_session.createDataFrame(data).toDF(\"date\",\"increment\")\n",
        "\n",
        "#Add Month value from another column\n",
        "df.select(df.date,df.increment,\n",
        "     expr(\"add_months(date,increment)\")\n",
        "  .alias(\"inc_date\")).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "v94SrZV-p31z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4:Giving Column Alias along with expr()\n",
        "# Providing alias using 'as'\n",
        "from pyspark.sql.functions import expr\n",
        "df.select(df.date,df.increment,\n",
        "     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n",
        "  ).show()\n",
        "\n",
        "#5:cast Function with expr()\n",
        "df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n",
        "  .printSchema()\n",
        "\n",
        "#6: Arithmetic operations\n",
        "df.select(df.date,df.increment,\n",
        "     expr(\"increment + 5 as new_increment\")\n",
        "  ).show()\n",
        "\n",
        "#7: Using Filter with expr()\n",
        "#Use expr()  to filter the rows\n",
        "from pyspark.sql.functions import expr\n",
        "data=[(100,2),(200,3000),(500,500)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\")\n",
        "df.filter(expr(\"col1 == col2\")).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wQpINKspqN64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark lit() – Add Literal or Constant to DataFrame\n",
        "\n",
        "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
        "columns= [\"EmpId\",\"Salary\"]\n",
        "df = spark_session.createDataFrame(data = data, schema = columns)\n",
        "\n",
        "from pyspark.sql.functions import col,lit\n",
        "\n",
        "df2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\n",
        "df2.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "df2.withColumn(\n",
        "    \"lit_value2\",\n",
        "    when((col(\"Salary\") >= 40000) & (col(\"Salary\") <= 50000), lit(\"100\"))\n",
        "    .otherwise(lit(\"200\"))\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "id": "lZOUuDaPrLQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994339be-862e-44a2-dda8-42f1ce67e7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+\n",
            "|EmpId|Salary|lit_value1|\n",
            "+-----+------+----------+\n",
            "|111  |50000 |1         |\n",
            "|222  |60000 |1         |\n",
            "|333  |40000 |1         |\n",
            "+-----+------+----------+\n",
            "\n",
            "+-----+------+----------+----------+\n",
            "|EmpId|Salary|lit_value1|lit_value2|\n",
            "+-----+------+----------+----------+\n",
            "|111  |50000 |1         |100       |\n",
            "|222  |60000 |1         |200       |\n",
            "|333  |40000 |1         |100       |\n",
            "+-----+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Convert String to Array Column\n",
        "#To convert a string column (StringType) to an array column (ArrayType) in PySpark, you can use the split() function from the pyspark.sql.functions module.\n",
        "#pyspark.sql.functions.split(str, pattern, limit=-1)\n",
        "#Consider below example\n",
        "\n",
        "# Data\n",
        "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
        "        (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
        "        (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
        "        (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
        "        (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
        "]\n",
        "\n",
        "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#PySpark Convert String to Array Column\n",
        "df.select(split(col(\"name\"),\",\").alias(\"NameArray\")).show(truncate=False)\n",
        "\n",
        "#Using split() on withColumn()\n",
        "# Splitting the \"name\" column into an array of first name, middle name, and last name\n",
        "df.withColumn(\"name_array\", split(df[\"name\"], \",\\s*\")).show(truncate=False)\n",
        "\n",
        "#Convert String to Array Column using SQL Query\n",
        "df.createOrReplaceTempView(\"PERSON\")\n",
        "spark_session.sql(\"select SPLIT(name,',') as NameArray from PERSON\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwE9r9NtU32a",
        "outputId": "c304b295-26e6-4533-9a5e-afa4f22f90be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-374200728.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  df.withColumn(\"name_array\", split(df[\"name\"], \",\\s*\")).show(truncate=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+\n",
            "|NameArray               |\n",
            "+------------------------+\n",
            "|[James,  A,  Smith]     |\n",
            "|[Michael,  Rose,  Jones]|\n",
            "|[Robert, K, Williams]   |\n",
            "|[Maria, Anne, Jones]    |\n",
            "|[Jen, Mary, Brown]      |\n",
            "+------------------------+\n",
            "\n",
            "+--------------------+--------+------+------+----------------------+\n",
            "|name                |dob_year|gender|salary|name_array            |\n",
            "+--------------------+--------+------+------+----------------------+\n",
            "|James, A, Smith     |2018    |M     |3000  |[James, A, Smith]     |\n",
            "|Michael, Rose, Jones|2010    |M     |4000  |[Michael, Rose, Jones]|\n",
            "|Robert,K,Williams   |2010    |M     |4000  |[Robert, K, Williams] |\n",
            "|Maria,Anne,Jones    |2005    |F     |4000  |[Maria, Anne, Jones]  |\n",
            "|Jen,Mary,Brown      |2010    |      |-1    |[Jen, Mary, Brown]    |\n",
            "+--------------------+--------+------+------+----------------------+\n",
            "\n",
            "+------------------------+\n",
            "|NameArray               |\n",
            "+------------------------+\n",
            "|[James,  A,  Smith]     |\n",
            "|[Michael,  Rose,  Jones]|\n",
            "|[Robert, K, Williams]   |\n",
            "|[Maria, Anne, Jones]    |\n",
            "|[Jen, Mary, Brown]      |\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark – Convert array column to a String\n",
        "#PySpark function concat_ws() (translates to concat with separator),\n",
        "from pyspark.sql.functions import concat_ws,col\n",
        "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
        "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
        "        (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
        "        (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#1:Convert an array of String to String column using concat_ws()\n",
        "df.withColumn(\"languagesAtSchool\",concat_ws(\",\",col(\"languagesAtSchool\"))).show(truncate=False)\n",
        "\n",
        "#2:Using PySpark SQL expression\n",
        "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
        "spark_session.sql(\"select name,concat_ws(',',languagesAtSchool) AS languagesAtSchool from ARRAY_STRING\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su_ptn-HZjz-",
        "outputId": "d5ac4d99-f2fd-4f43-b2d9-69500e8fa4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+------------+\n",
            "|name            |languagesAtSchool|currentState|\n",
            "+----------------+-----------------+------------+\n",
            "|James,,Smith    |Java,Scala,C++   |CA          |\n",
            "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
            "|Robert,,Williams|CSharp,VB        |NV          |\n",
            "+----------------+-----------------+------------+\n",
            "\n",
            "+----------------+-----------------+\n",
            "|name            |languagesAtSchool|\n",
            "+----------------+-----------------+\n",
            "|James,,Smith    |Java,Scala,C++   |\n",
            "|Michael,Rose,   |Spark,Java,C++   |\n",
            "|Robert,,Williams|CSharp,VB        |\n",
            "+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pyspark – Get substring() from a column\n",
        "#substring(str, pos, len)\n",
        "\n",
        "from pyspark.sql.functions import substring\n",
        "# Create Sample Data\n",
        "data = [(1,\"20200828\"),(2,\"20180525\")]\n",
        "columns=[\"id\",\"date\"]\n",
        "df=spark_session.createDataFrame(data,columns)\n",
        "\n",
        "df.withColumn(\"year\",substring(col(\"date\"),1,4))\\\n",
        "  .withColumn(\"month\",substring(col(\"date\"),5,2))\\\n",
        "  .withColumn(\"day\",substring(col(\"date\"),7,2)).show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po7IRbFYZd1C",
        "outputId": "4f5a02fc-c1ac-447f-f8f3-2f61e26a0657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----+-----+---+\n",
            "|id |date    |year|month|day|\n",
            "+---+--------+----+-----+---+\n",
            "|1  |20200828|2020|08   |28 |\n",
            "|2  |20180525|2018|05   |25 |\n",
            "+---+--------+----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark- Translate - By using translate() string function you can replace character by character of DataFrame column value.\n",
        "#Using translate to replace character by character\n",
        "from pyspark.sql.functions import translate\n",
        "df.withColumn('address', translate('address', '123', 'ABC')).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "4DWMF3YKbDaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using overlay() Function - Replace column value with a string value from another column.\n",
        "from pyspark.sql.functions import overlay\n",
        "df = spark_session.createDataFrame([(\"ABCDE_XYZ\", \"FGHI\")], (\"col1\", \"col2\"))\n",
        "df.select(overlay(\"col1\", \"col2\", 7).alias(\"overlayed\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IwN9tDvdqLn",
        "outputId": "7616bb83-c7bc-4ac3-ffe9-6d6b00de7eee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|overlayed |\n",
            "+----------+\n",
            "|ABCDE_FGHI|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASRjXjAcfzdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}