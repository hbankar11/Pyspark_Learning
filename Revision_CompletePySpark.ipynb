{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXc9+Vm4EZhnZJecLZuKy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbankar11/Pyspark_Learning/blob/main/Revision_CompletePySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZIt4i3jfS1Z",
        "outputId": "0a6aa007-7dfd-4550-8197-08a0b4b9f8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Just Checking PySpark is Installed or not\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "BUUaHRD0f4kK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Spark Session\n",
        "spark_session = SparkSession.builder.appName(\"Revision_CompletePySpark\").getOrCreate()"
      ],
      "metadata": {
        "id": "g_Iw0d7YhhPM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create SparkContext in PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_session = SparkSession.builder.appName(\"Revision_PySpark\").getOrCreate()\n",
        "print(spark_session.sparkContext)\n",
        "print(\"Spark App Name : \" + spark_session.sparkContext.appName)"
      ],
      "metadata": {
        "id": "HmUp0sLlh-vJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5012e2-3800-49dd-f653-01d49213dc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=Revision_CompletePySpark>\n",
            "Spark App Name : Revision_CompletePySpark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop PySpark SparkContext\n",
        "#When PySpark executes this statement, it logs the message INFO SparkContext: Successfully stopped SparkContext to console or to a log file.\n",
        "spark_session.sparkContext.stop()"
      ],
      "metadata": {
        "id": "Dty405JtkM15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create RDD using parallelize\n",
        "data = [1,2,3,4,5,6,7,8,9,10]\n",
        "rdd = spark_session.sparkContext.parallelize(data)\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wTnOT6Ym6wg",
        "outputId": "5906eeda-0809-4121-8af2-f1c3215f386d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create RDD using sparkContext.textFile()\n",
        "rdd2 = spark_session.sparkContext.textFile(\"/path/textFile.txt\")\n"
      ],
      "metadata": {
        "id": "nFnL_J1WpXsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create RDD using sparkContext.wholeTextFiles()\n",
        "rdd3 = spark_session.sparkContext.wholeTextFiles(\"<path>\")"
      ],
      "metadata": {
        "id": "0w6uNPCwpnNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create empty RDD using sparkContext.emptyRDD\n",
        "emptyRDD = spark_session.sparkContext.emptyRDD"
      ],
      "metadata": {
        "id": "oIy_Ycamp4D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating empty RDD with partition\n",
        "emptyRDDPartition = spark_session.sparkContext.parallelize([],10)"
      ],
      "metadata": {
        "id": "-6c-4qsPqXEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD Partitions\n",
        "#Get Partition count\n",
        "print(\"Initial partition count : \" + str(rdd.getNumPartitions()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwn4nQlOqmVY",
        "outputId": "355bacbc-cb81-488e-f463-6e98629961ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial partition count : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set partition Manually\n",
        "rddPartition = spark_session.sparkContext.parallelize([1,2,3],10)\n",
        "rddPartition.collect()\n",
        "rddPartition.getNumPartitions() #10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctD_mX26rPoy",
        "outputId": "1892b915-234a-4099-fbae-dd8d114da9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Repartition and coalesce\n",
        "reparRdd = rdd.repartition(4)\n",
        "#print(\"re-partition count : \"+str(reparRdd.getNumPartitions()))\n",
        "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
        "\n",
        "coalRdd = rdd.coalesce(2)\n",
        "print(\"coalesce count:\"+str(coalRdd.getNumPartitions()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk8dgIshSa4W",
        "outputId": "34c9ffdd-4c22-4663-f4c2-596d632a99cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-partition count:4\n",
            "coalesce count:2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatMap() : flattens all those sequences into a single RDD\n",
        "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n"
      ],
      "metadata": {
        "id": "Zr4gIMaGTdFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#map() : It allows you to apply a function to each element of an RDD, producing a new RDD with the transformed elements.\n",
        "# Add a new element with value 1 to each word\n",
        "rdd3 = rdd2.map(lambda x: (x,1))\n"
      ],
      "metadata": {
        "id": "3z6iIsBZUila"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reduceByKey() : combines the values associated with each key using the provided function.\n",
        "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n"
      ],
      "metadata": {
        "id": "cswM5wAoVVkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sortByKey() : transformation arranges the elements of an RDD based on their keys.\n",
        "# Using sortByKey()\n",
        "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
        "\n",
        "# Print rdd5 result to console\n",
        "print(rdd5.collect())"
      ],
      "metadata": {
        "id": "gbdwKcbUWsy3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "224df580-d040-42b0-88ed-d45909e02dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 12) (9a6fed870004 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3029351147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sortByKey() : transformation arranges the elements of an RDD based on their keys.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Using sortByKey()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrdd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print rdd5 result to console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \"\"\"\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \"\"\"\n\u001b[0;32m-> 2291\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2292\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 12) (9a6fed870004 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n                ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-698648028.py\", line 2, in <lambda>\nAttributeError: 'int' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD action:\n",
        "#1: count() => Returns the number of records in an RDD\n",
        "print(\"Count : \"+str(rdd.count()))\n",
        "\n",
        "#2: first() => Returns the first record.\n",
        "firstRec = rdd.first()\n",
        "print(\"First Record : \"+str(firstRec))\n",
        "\n",
        "#3:max() => Returns maximum record\n",
        "datMax = rdd.max()\n",
        "print(\"Maximum Record : \"+str(datMax))\n",
        "\n",
        "#4: reduce() => Reduces the records to single, we can use this to count or sum.\n",
        "#totalWordCount = rdd.reduce(lambda a,b: (a[0]+b[0],a[1]))\n",
        "#print(\"dataReduce Record : \"+str(totalWordCount[0]))\n",
        "\n",
        "#5: take() =>  Returns the record specified as an argument.\n",
        "data = rdd.take(3)\n",
        "print(\"Element at position 3 :\"+str(data))\n",
        "\n",
        "#6: saveAsTextFile() => Using saveAsTestFile action, we can write the RDD to a text file.\n",
        "rdd.saveAsTextFile(\"/tmp/wordCount\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF67bdw-XsG1",
        "outputId": "502681b3-fb23-41e0-9621-c2f5974bf8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count : 10\n",
            "First Record : 1\n",
            "Maximum Record : 10\n",
            "Element at position 3 :[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating RDD from DataFrame and vice-versa\n",
        "# Converts RDD to DataFrame\n",
        "dfFromRDD1 = rdd.toDF()\n",
        "\n",
        "# Converts RDD to DataFrame with column names\n",
        "dfFromRDD2 = rdd.toDF(\"col1\",\"col2\")\n",
        "\n",
        "# using createDataFrame() - Convert DataFrame to RDD\n",
        "df = spark_session.createDataFrame(rdd).toDF(\"col1\",\"col2\")\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "rdd = df.rdd\n"
      ],
      "metadata": {
        "id": "4yOGdCw6fwZf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "13de53be-ca1d-42d9-ab81-b9d56e3033e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-229215165.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Creating RDD from DataFrame and vice-versa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Converts RDD to DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfFromRDD1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Converts RDD to DataFrame with column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             schema = _infer_schema(\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_INFER_SCHEMA_FOR_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataframe\n",
        "#1: Create empty RDD in Pyspark\n",
        "emptyRDD = spark_session.sparkContext.emptyRDD()\n",
        "print(emptyRDD)\n",
        "\n",
        "#2: Alternative method to create empty RDD\n",
        "emptyRDD1 = spark_session.sparkContext.parallelize([])\n",
        "print(emptyRDD1)\n",
        "\n",
        "#3:Create Empty DataFrame with Schema (StructType)\n",
        "from pyspark.sql.types import StructField,StructType,StringType\n",
        "schema = StructType([\n",
        "    StructField('firstname',StringType(),True),\n",
        "    StructField('middlename',StringType(),True),\n",
        "    StructField('lastname',StringType(),True)\n",
        "])\n",
        "\n",
        "emptyDF = spark_session.createDataFrame(emptyRDD,schema)\n",
        "emptyDF.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFUIqbtsgJp1",
        "outputId": "f3ddd733-8734-40ea-e82b-5b808e09aa52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmptyRDD[22] at emptyRDD at NativeMethodAccessorImpl.java:0\n",
            "ParallelCollectionRDD[23] at readRDDFromFile at PythonRDD.scala:289\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Empty RDD to DataFrame\n",
        "df = emptyRDD.toDF(schema)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKYWeEGCrFVR",
        "outputId": "234bf8a5-2355-48ee-bcf8-9657d83c544b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Empty DataFrame with Schema.\n",
        "df = spark_session.createDataFrame([],schema)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pIz-MJ_rycx",
        "outputId": "dc1ed21b-2491-4433-bc57-191d259f0dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Empty DataFrame without Schema (no columns)\n",
        "df = spark_session.createDataFrame([],StructType([]))\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "---p5pVOsGVF",
        "outputId": "2474fafd-6193-4190-f955-88d7b18c6824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create pyspark RDD\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark_session.sparkContext.parallelize(dept)\n",
        "rdd.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqc6g5_DscIL",
        "outputId": "bd866370-ae00-4f54-933a-43317c4d1bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Pyspark rdd to dataframe\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vT_WzkDtN9g",
        "outputId": "ddc6c265-b9f7-4291-ab25-9860892d6be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-QSiHffthnu",
        "outputId": "ac29185b-ab83-4c6f-e24e-8f648bee6e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Pyspark createDataFrame() function\n",
        "deptDF = spark_session.createDataFrame(rdd,schema=deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z07XiuEVuSSG",
        "outputId": "71e61c4f-4cce-4493-e681-fe5a676f97d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using createDataFrame() with StructType schema\n",
        "from pyspark.sql.types import StringType,StructType,StructField,StringType,IntegerType\n",
        "\n",
        "deptSchema = StructType([\n",
        "    StructField(\"dept_name\",StringType(),True),\n",
        "    StructField(\"dept_id\",IntegerType(),True),\n",
        "])\n",
        "deptDF1 = spark_session.createDataFrame(rdd,schema=deptSchema)\n",
        "deptDF1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFrU2CWSvHmz",
        "outputId": "ce27bf8d-b058-46a4-b9ab-773f18a0b76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pyspark Dataframe to Pandas Dataframe\n",
        "# Consider below pyspark dataframe\n",
        "\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
        "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
        "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
        "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
        "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
        "\n",
        "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
        "pysparkDF = spark_session.createDataFrame(data = data, schema = columns)\n",
        "#pysparkDF.printSchema()\n",
        "#pysparkDF.show(truncate=False)\n",
        "\n",
        "#Convert above pyspark dataframe to Pandas\n",
        "pandasDF = pysparkDF.toPandas()\n",
        "print(pandasDF)"
      ],
      "metadata": {
        "id": "d5yplcqUyOgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24faf31f-41c0-4bd1-a208-a8e96ecbf2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  first_name middle_name last_name    dob gender  salary\n",
            "0      James                 Smith  36636      M   60000\n",
            "1    Michael        Rose            40288      M   70000\n",
            "2     Robert              Williams  42114         400000\n",
            "3      Maria        Anne     Jones  39192      F  500000\n",
            "4        Jen        Mary     Brown             F       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark show()  Display DataFrame Contents in Table\n",
        "# Default - displays 20 rows and\n",
        "# 20 charactes from column value\n",
        "df.show()\n",
        "\n",
        "#Display full column contents\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Display 2 rows and full column contents\n",
        "df.show(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)"
      ],
      "metadata": {
        "id": "uhMgZxEPPkse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Below example\n",
        "columns = [\"Seqno\",\"Quote\"]\n",
        "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
        "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
        "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
        "    (\"4\", \"Be cool.\")]\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "df.show()\n",
        "df.show(truncate=False)\n",
        "df(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6K2QY-qQCqI",
        "outputId": "c6b684b6-e3e2-4b2e-ff02-4465f789a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------------------------------------------------------------------+\n",
            "|Seqno|Quote                                                                        |\n",
            "+-----+-----------------------------------------------------------------------------+\n",
            "|1    |Be the change that you wish to see in the world                              |\n",
            "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
            "|3    |The purpose of our lives is to be happy.                                     |\n",
            "|4    |Be cool.                                                                     |\n",
            "+-----+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark StructType & StructField Explained with Examples\n",
        "#Using PySpark StructType & StructField with DataFrame\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "  ]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"First_Name\",StringType(),True),\n",
        "    StructField(\"Middle_Name\",StringType(),True),\n",
        "    StructField(\"Last_Name\",StringType(),True),\n",
        "    StructField(\"ID\",StringType(),True),\n",
        "    StructField(\"Gender\",StringType(),True),\n",
        "    StructField(\"Salary\",IntegerType(),True)\n",
        "\n",
        "])\n",
        "\n",
        "df = spark_session.createDataFrame(data,schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ONbC5VlkSLfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1636b810-bc06-45d2-a046-af4f7b724b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- First_Name: string (nullable = true)\n",
            " |-- Middle_Name: string (nullable = true)\n",
            " |-- Last_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "+----------+-----------+---------+-----+------+------+\n",
            "|First_Name|Middle_Name|Last_Name|ID   |Gender|Salary|\n",
            "+----------+-----------+---------+-----+------+------+\n",
            "|James     |           |Smith    |36636|M     |3000  |\n",
            "|Michael   |Rose       |         |40288|M     |4000  |\n",
            "|Robert    |           |Williams |42114|M     |4000  |\n",
            "|Maria     |Anne       |Jones    |39192|F     |4000  |\n",
            "|Jen       |Mary       |Brown    |     |F     |-1    |\n",
            "+----------+-----------+---------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining schema using nested StructType\n",
        "from pyspark.sql.types import StructType,StructField,StringType\n",
        "structureData = [\n",
        "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
        "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
        "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\",StructType([\n",
        "        StructField(\"FirstName\",StringType(),True),\n",
        "        StructField(\"MiddleName\",StringType(),True),\n",
        "        StructField(\"LastName\",StringType(),True),]),True),\n",
        "    StructField(\"ID\",StringType(),True),\n",
        "    StructField(\"Gender\",StringType(),True),\n",
        "    StructField(\"Salary\",StringType(),True)\n",
        "])\n",
        "\n",
        "df = spark_session.createDataFrame(structureData,schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecYN3Bq_Cfii",
        "outputId": "7772d589-e7ac-4d11-d7dc-56adbd9fff14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: struct (nullable = true)\n",
            " |    |-- FirstName: string (nullable = true)\n",
            " |    |-- MiddleName: string (nullable = true)\n",
            " |    |-- LastName: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Salary: string (nullable = true)\n",
            "\n",
            "+--------------------+-----+------+------+\n",
            "|Name                |ID   |Gender|Salary|\n",
            "+--------------------+-----+------+------+\n",
            "|{James, , Smith}    |36636|M     |3100  |\n",
            "|{Michael, Rose, }   |40288|M     |4300  |\n",
            "|{Robert, , Williams}|42114|M     |1400  |\n",
            "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
            "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
            "+--------------------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Create Column Class Object\n",
        "from pyspark.sql.functions import lit\n",
        "colObj = lit(\"sparkbyexamples.com\")"
      ],
      "metadata": {
        "id": "Hyvxlhm2K3V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "data=[(\"James\",23),(\"Ann\",40)]\n",
        "df=spark_session.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
        "df.printSchema()\n",
        "#root\n",
        "# |-- name.fname: string (nullable = true)\n",
        "# |-- gender: long (nullable = true)\n",
        "\n",
        "# Using DataFrame object (df)\n",
        "df.select(df.gender).show(truncate=False)\n",
        "df.select(df[\"gender\"]).show(truncate=False)\n",
        "\n",
        "#Accessing column name with dot operator\n",
        "df.select(df[\"`name.fname`\"]).show(truncate=False)\n",
        "\n",
        "#Using SQL col() function\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"gender\")).show(truncate=False)\n",
        "\n",
        "#Accessing column name with dot (with backticks)\n",
        "df.select(col(\"`name.fname`\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEU8LV8CMDkN",
        "outputId": "723d5666-c813-4dbc-d0fc-585eb18cca4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name.fname: string (nullable = true)\n",
            " |-- gender: long (nullable = true)\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|23    |\n",
            "|40    |\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|23    |\n",
            "|40    |\n",
            "+------+\n",
            "\n",
            "+----------+\n",
            "|name.fname|\n",
            "+----------+\n",
            "|James     |\n",
            "|Ann       |\n",
            "+----------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|23    |\n",
            "|40    |\n",
            "+------+\n",
            "\n",
            "+----------+\n",
            "|name.fname|\n",
            "+----------+\n",
            "|James     |\n",
            "|Ann       |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create DataFrame with struct using Row class\n",
        "from pyspark.sql import Row\n",
        "\n",
        "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
        "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
        "\n",
        "df = spark_session.createDataFrame(data)\n",
        "df.printSchema()\n",
        "\n",
        "#Access struct column\n",
        "df.select(df.prop.hair).show(truncate=False)\n",
        "df.select(df[\"prop.hair\"]).show(truncate=False)\n",
        "df.select(col(\"prop.hair\")).show(truncate=False)\n",
        "\n",
        "#Access all columns from struct\n",
        "df.select(col(\"prop.*\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSGbITT3Owlg",
        "outputId": "9505eb75-9272-4491-ad26-284df633f099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- prop: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            "\n",
            "+---------+\n",
            "|prop.hair|\n",
            "+---------+\n",
            "|black    |\n",
            "|grey     |\n",
            "+---------+\n",
            "\n",
            "+-----+\n",
            "|hair |\n",
            "+-----+\n",
            "|black|\n",
            "|grey |\n",
            "+-----+\n",
            "\n",
            "+-----+\n",
            "|hair |\n",
            "+-----+\n",
            "|black|\n",
            "|grey |\n",
            "+-----+\n",
            "\n",
            "+-----+-----+\n",
            "|hair |eye  |\n",
            "+-----+-----+\n",
            "|black|blue |\n",
            "|grey |black|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark_session.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "#Arthmetic operations\n",
        "df.select(col(\"col1\") + col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") - col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") * col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") / col(\"col2\")).show(truncate=False)\n",
        "\n",
        "df.select(col(\"col1\") % col(\"col2\")).show(truncate=False)\n",
        "\n",
        "df.select(col(\"col1\") == col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") > col(\"col2\")).show(truncate=False)\n",
        "df.select(col(\"col1\") < col(\"col2\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZznNH8_Swhn",
        "outputId": "35e86a26-b0b1-4341-811f-dd27cc3be030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|(col1 + col2)|\n",
            "+-------------+\n",
            "|102          |\n",
            "|203          |\n",
            "|304          |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 - col2)|\n",
            "+-------------+\n",
            "|98           |\n",
            "|197          |\n",
            "|296          |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 * col2)|\n",
            "+-------------+\n",
            "|200          |\n",
            "|600          |\n",
            "|1200         |\n",
            "+-------------+\n",
            "\n",
            "+-----------------+\n",
            "|(col1 / col2)    |\n",
            "+-----------------+\n",
            "|50.0             |\n",
            "|66.66666666666667|\n",
            "|75.0             |\n",
            "+-----------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 % col2)|\n",
            "+-------------+\n",
            "|0            |\n",
            "|2            |\n",
            "|0            |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 = col2)|\n",
            "+-------------+\n",
            "|false        |\n",
            "|false        |\n",
            "|false        |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 > col2)|\n",
            "+-------------+\n",
            "|true         |\n",
            "|true         |\n",
            "|true         |\n",
            "+-------------+\n",
            "\n",
            "+-------------+\n",
            "|(col1 < col2)|\n",
            "+-------------+\n",
            "|false        |\n",
            "|false        |\n",
            "|false        |\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr,col\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#alias()  Sets name to Column\n",
        "df.select(df.fname.alias(\"First_Name\"), \\\n",
        "          df.lname.alias(\"Last_Name\")).show(truncate=False)\n",
        "\n",
        "#Another Example\n",
        "df.select(expr(\"fname ||' '|| lname\").alias(\"Full_Name\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0hHcEJS7jBd",
        "outputId": "42d1ba42-9bba-4f22-fd13-8345f9f3c1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|First_Name|Last_Name|\n",
            "+----------+---------+\n",
            "|James     |Bond     |\n",
            "|Ann       |Varsa    |\n",
            "|Tom Cruise|XXX      |\n",
            "|Tom Brand |NULL     |\n",
            "+----------+---------+\n",
            "\n",
            "+--------------+\n",
            "|Full_Name     |\n",
            "+--------------+\n",
            "|James Bond    |\n",
            "|Ann Varsa     |\n",
            "|Tom Cruise XXX|\n",
            "|NULL          |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asc() & desc()  Sort the DataFrame columns by Ascending or Descending order.\n",
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()\n"
      ],
      "metadata": {
        "id": "gngBCHNG-sul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1b25ee-6c35-4282-ca2c-5d3d8f7b1e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|       Ann|Varsa|200|     F|\n",
            "|     James| Bond|100|  NULL|\n",
            "| Tom Brand| NULL|400|     M|\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "+----------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "| Tom Brand| NULL|400|     M|\n",
            "|     James| Bond|100|  NULL|\n",
            "|       Ann|Varsa|200|     F|\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cast() & astype()  Used to convert the data Type.\n",
        "df.select(df.fname,df.id.cast('int')).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtlU3an_FJf",
        "outputId": "5b6e3a1b-b5ef-4275-cee1-be9d70677173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- fname: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#between()  Returns a Boolean expression when a column values in between lower and upper bound.\n",
        "df.filter(df.id.between(100,300)).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2jdZtp__tf",
        "outputId": "ef648812-4302-401e-8087-f724bb2f4f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---+------+\n",
            "|fname|lname|id |gender|\n",
            "+-----+-----+---+------+\n",
            "|James|Bond |100|NULL  |\n",
            "|Ann  |Varsa|200|F     |\n",
            "+-----+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contains() -\n",
        "df.filter(df.fname.contains(\"Cruise\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "TnIJz7KnAhH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71713dd9-3450-4d68-ae9a-6ec3d0658be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# startsWith and endsWith()\n",
        "df.filter(df.fname.startswith(\"T\")).show(truncate=False)\n",
        "df.filter(df.fname.endswith(\"Cruise\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfzSG1UJSms8",
        "outputId": "ec898cee-9bed-4a66-c07c-a30155b34ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "|Tom Brand |NULL |400|M     |\n",
            "+----------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# isNull & isNotNull()  Checks if the DataFrame column has NULL or non NULL values.\n",
        "df.filter(df.lname.isNull()).show(truncate=False)\n",
        "df.filter(df.lname.isNotNull()).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M21m_hZUaAMd",
        "outputId": "3f4cc377-e8ab-496b-e1bf-9e7c26bb3e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+---+------+\n",
            "|fname    |lname|id |gender|\n",
            "+---------+-----+---+------+\n",
            "|Tom Brand|NULL |400|M     |\n",
            "+---------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|fname     |lname|id |gender|\n",
            "+----------+-----+---+------+\n",
            "|James     |Bond |100|NULL  |\n",
            "|Ann       |Varsa|200|F     |\n",
            "|Tom Cruise|XXX  |400|      |\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# like() & rlike()  Similar to SQL LIKE expression\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.fname.like(\"%om%\")) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq86a9JEa6eJ",
        "outputId": "ef153807-0caf-41f7-d421-5f245cb3f7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+\n",
            "|fname     |lname|id |\n",
            "+----------+-----+---+\n",
            "|Tom Cruise|XXX  |400|\n",
            "|Tom Brand |NULL |400|\n",
            "+----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# substr()  Returns a Column after getting sub string from the Column\n",
        "df.select(df.fname.substr(1,2).alias(\"substr\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uFDQGgubU8J",
        "outputId": "d663666a-9362-44a8-865e-6c2c5f8fa508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|substr|\n",
            "+------+\n",
            "|Ja    |\n",
            "|An    |\n",
            "|To    |\n",
            "|To    |\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when() & otherwise()  It is similar to SQL Case When, executes sequence of expressions until it matches the condition and returns a value when match.\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender == 'M','Male') \\\n",
        "                           .when(df.gender == 'F','Female') \\\n",
        "                           .when(df.gender == None, \"\") \\\n",
        "                           .otherwise(df.gender).alias(\"gender\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7DaE0oPgj8m",
        "outputId": "bf1b067c-09f8-4d69-f59d-5a167cd73f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+\n",
            "|fname     |lname|gender|\n",
            "+----------+-----+------+\n",
            "|James     |Bond |NULL  |\n",
            "|Ann       |Varsa|Female|\n",
            "|Tom Cruise|XXX  |      |\n",
            "|Tom Brand |NULL |Male  |\n",
            "+----------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# isin()  Check if value presents in a List.\n",
        "li = ['100','200']\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.id.isin(li)) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uedpshPpjc3T",
        "outputId": "9149372e-82c9-4903-b5bd-1828bc6fda63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---+\n",
            "|fname|lname|id |\n",
            "+-----+-----+---+\n",
            "|James|Bond |100|\n",
            "|Ann  |Varsa|200|\n",
            "+-----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getField()  To get the value by key from MapType column and by struct child name from StructType column\n",
        "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
        "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
        "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
        "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
        "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
        "\n",
        "schema = StructType([\n",
        "        StructField('name', StructType([\n",
        "            StructField('fname', StringType(), True),\n",
        "            StructField('lname', StringType(), True)])),\n",
        "        StructField('languages', ArrayType(StringType()),True),\n",
        "        StructField('properties', MapType(StringType(),StringType()),True)\n",
        "])\n",
        "\n",
        "df = spark_session.createDataFrame(data,schema)\n",
        "df.select(df.properties.getField(\"hair\")).show(truncate=False)\n",
        "\n",
        "df.select(df.name.getField(\"fname\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "n_HwNgkPkCLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6604d1-beb4-4bb3-df12-36072d4813e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|properties[hair]|\n",
            "+----------------+\n",
            "|black           |\n",
            "|brown           |\n",
            "|red             |\n",
            "|black           |\n",
            "+----------------+\n",
            "\n",
            "+----------+\n",
            "|name.fname|\n",
            "+----------+\n",
            "|James     |\n",
            "|Ann       |\n",
            "|Tom Cruise|\n",
            "|Tom Brand |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getItem()  To get the value by index from MapType or ArrayTupe & ny key for MapType column.\n",
        "\n",
        "#getItem() used with ArrayType\n",
        "df.select(df.languages.getItem(1)).show()\n",
        "\n",
        "#getItem() used with MapType\n",
        "df.select(df.properties.getItem(\"hair\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSYG7uJ_Wi1T",
        "outputId": "95008852-ce58-4f33-917d-91b7680137dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|languages[1]|\n",
            "+------------+\n",
            "|          C#|\n",
            "|      Python|\n",
            "|       Scala|\n",
            "|        Ruby|\n",
            "+------------+\n",
            "\n",
            "+----------------+\n",
            "|properties[hair]|\n",
            "+----------------+\n",
            "|           black|\n",
            "|           brown|\n",
            "|             red|\n",
            "|           black|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select-columns-from-pyspark-dataframe\n",
        "# consider below example\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "]\n",
        "\n",
        "# Column names\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "df.printSchema()\n",
        "\n",
        "#1: Select Single & Multiple Columns From PySpark\n",
        "df.select(\"firstname\",\"lastname\").show(truncate=False)\n",
        "df.select(df.firstname,df.lastname).show(truncate=False)\n",
        "df.select(df[\"firstname\"],df[\"lastname\"]).show(truncate=False)\n",
        "\n",
        "#1.1 using col()\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"firstname\"),col(\"lastname\")).show(truncate=False)\n",
        "\n",
        "# Select columns by regular expression\n",
        "df.select(df.colRegex(\"`^.*name*`\")).show()\n",
        "\n",
        "#2. Select All Columns From List\n",
        "df.select(*columns).show(truncate=False)\n",
        "\n",
        "#select All columns\n",
        "df.select([col for col in df.columns]).show(truncate=False)\n",
        "df.select(\"*\").show(truncate=False)\n",
        "\n",
        "#3. Select Columns by Index\n",
        "#3.1 Selects first 3 columns and top 3 rows\n",
        "df.select(df.columns[:3]).show(3,truncate=False)\n",
        "\n",
        "#3.2 Selects columns 2 to 4  and top 3 rows\n",
        "df.select(df.columns[2:4]).show(3,truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ghKQQFcLKF",
        "outputId": "f8e44449-ea48-4081-a7eb-7277a893595b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Michael  |Rose    |\n",
            "|Robert   |Williams|\n",
            "|Maria    |Jones   |\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+\n",
            "|firstname|lastname|country|\n",
            "+---------+--------+-------+\n",
            "|James    |Smith   |USA    |\n",
            "|Michael  |Rose    |USA    |\n",
            "|Robert   |Williams|USA    |\n",
            "+---------+--------+-------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------+-----+\n",
            "|country|state|\n",
            "+-------+-----+\n",
            "|USA    |CA   |\n",
            "|USA    |NY   |\n",
            "|USA    |CA   |\n",
            "+-------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.4 Select Nested Struct Columns from PySpark\n",
        "from pyspark.sql.types import StructType,StructField,StringType\n",
        "# Create DataFrame with nested columns\n",
        "data = [\n",
        "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\",StructType([\n",
        "        StructField(\"Firstname\",StringType(),True),\n",
        "        StructField(\"Middlename\",StringType(),True),\n",
        "        StructField(\"Lastname\",StringType(),True),\n",
        "    ]),True),\n",
        "    StructField(\"State\",StringType(),True),\n",
        "    StructField(\"gender\",StringType(),True),\n",
        "])\n",
        "\n",
        "df2 = spark_session.createDataFrame(data,schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "#1: To select struct column name\n",
        "df2.select(\"name\").show(truncate=False)\n",
        "\n",
        "#2: In order to select the specific column from a nested struct, you need to explicitly qualify the nested struct column name.\n",
        "df2.select(\"Name.Firstname\",\"Name.Lastname\").show(truncate=False)\n",
        "\n",
        "#3: To get all columns from the struct column, specify name.*\n",
        "df2.select(\"Name.*\").show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C23_1NoppPwi",
        "outputId": "19b9d2a1-f2ac-487e-9bf2-f46e1c166f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: struct (nullable = true)\n",
            " |    |-- Firstname: string (nullable = true)\n",
            " |    |-- Middlename: string (nullable = true)\n",
            " |    |-- Lastname: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|Name                  |State|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n",
            "+----------------------+\n",
            "|name                  |\n",
            "+----------------------+\n",
            "|{James, NULL, Smith}  |\n",
            "|{Anna, Rose, }        |\n",
            "|{Julia, , Williams}   |\n",
            "|{Maria, Anne, Jones}  |\n",
            "|{Jen, Mary, Brown}    |\n",
            "|{Mike, Mary, Williams}|\n",
            "+----------------------+\n",
            "\n",
            "+---------+--------+\n",
            "|Firstname|Lastname|\n",
            "+---------+--------+\n",
            "|James    |Smith   |\n",
            "|Anna     |        |\n",
            "|Julia    |Williams|\n",
            "|Maria    |Jones   |\n",
            "|Jen      |Brown   |\n",
            "|Mike     |Williams|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+----------+--------+\n",
            "|Firstname|Middlename|Lastname|\n",
            "+---------+----------+--------+\n",
            "|James    |NULL      |Smith   |\n",
            "|Anna     |Rose      |        |\n",
            "|Julia    |          |Williams|\n",
            "|Maria    |Anne      |Jones   |\n",
            "|Jen      |Mary      |Brown   |\n",
            "|Mike     |Mary      |Williams|\n",
            "+---------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Collect()  Retrieve data from DataFrame\n",
        "# Consider below example:\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "df = spark_session.createDataFrame(dept,deptColumns)\n",
        "df.collect()\n",
        "\n",
        "#Return value of first row, first column\n",
        "df.collect()[0][0]\n",
        "\n",
        "#returns the first element in an array (1st row).\n",
        "df.collect()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys65znf0ybWy",
        "outputId": "373ef088-671a-48e0-f06f-2d61c6c84c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(dept_name='Finance', dept_id=10)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark withColumn() Usage with Examples\n",
        "# consider below example\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "\n",
        "#1: Change DataType using PySpark withColumn()\n",
        "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).printSchema()\n",
        "\n",
        "#2: Update The Value of an Existing Column\n",
        "df.withColumn(\"salary\",col(\"salary\")*100).show(truncate=False)\n",
        "\n",
        "#3: Create a Column from an Existing\n",
        "df.withColumn(\"copiedColumn\",col(\"salary\")* 1).show(truncate=False)\n",
        "\n",
        "#4: Add a New Column using withColumn()\n",
        "from pyspark.sql.functions import lit\n",
        "df.withColumn(\"country\",lit(\"USA\")) \\\n",
        "  .withColumn(\"AnotherValue\",lit(\"another_value\")) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "#5. Rename Column Name\n",
        "df.withColumnRenamed(\"gender\",\"sex\").printSchema()\n",
        "\n",
        "#6. Drop Column From PySpark DataFrame\n",
        "df.drop(\"gender\").printSchema()\n",
        "\n"
      ],
      "metadata": {
        "id": "bnfWUT-rgSZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4b4b04-af0e-4812-8a9d-afc351ea9553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|James    |          |Smith   |1991-04-01|M     |300000|\n",
            "|Michael  |Rose      |        |2000-05-19|M     |400000|\n",
            "|Robert   |          |Williams|1978-09-05|M     |400000|\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |400000|\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-100  |\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n",
            "+---------+----------+--------+----------+------+------+------------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|copiedColumn|\n",
            "+---------+----------+--------+----------+------+------+------------+\n",
            "|James    |          |Smith   |1991-04-01|M     |3000  |3000        |\n",
            "|Michael  |Rose      |        |2000-05-19|M     |4000  |4000        |\n",
            "|Robert   |          |Williams|1978-09-05|M     |4000  |4000        |\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |4000        |\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |-1          |\n",
            "+---------+----------+--------+----------+------+------+------------+\n",
            "\n",
            "+---------+----------+--------+----------+------+------+-------+-------------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|country|AnotherValue |\n",
            "+---------+----------+--------+----------+------+------+-------+-------------+\n",
            "|James    |          |Smith   |1991-04-01|M     |3000  |USA    |another_value|\n",
            "|Michael  |Rose      |        |2000-05-19|M     |4000  |USA    |another_value|\n",
            "|Robert   |          |Williams|1978-09-05|M     |4000  |USA    |another_value|\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |USA    |another_value|\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |USA    |another_value|\n",
            "+---------+----------+--------+----------+------+------+-------+-------------+\n",
            "\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark withColumnRenamed to Rename Column on DataFrame\n",
        "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
        "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
        "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
        "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
        "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "schema = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\n",
        "             StructField('middlename', StringType(), True),\n",
        "             StructField('lastname', StringType(), True)\n",
        "             ])),\n",
        "         StructField('dob', StringType(), True),\n",
        "         StructField('gender', StringType(), True),\n",
        "         StructField('salary', IntegerType(), True)\n",
        "         ])\n",
        "df = spark_session.createDataFrame(dataDF,schema)\n",
        "df.printSchema()\n",
        "\n",
        "#1. PySpark withColumnRenamed  To rename DataFrame column name\n",
        "df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n",
        "\n",
        "#2. PySpark withColumnRenamed  To rename multiple columns\n",
        "df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
        "  .withColumnRenamed(\"gender\",\"sex\").printSchema()\n",
        "\n",
        "#3. Using PySpark StructType  To rename a nested column in Dataframe\n",
        "schema2 = StructType([\n",
        "    StructField(\"fname\",StringType()),\n",
        "    StructField(\"middlename\",StringType()),\n",
        "    StructField(\"lname\",StringType())])\n",
        "\n",
        "df.select(col(\"name\").cast(schema2), \\\n",
        "          col(\"dob\"), col(\"gender\"), col(\"salary\")) \\\n",
        "  .printSchema()\n",
        "\n",
        "#4. Using Select  To rename nested elements.\n",
        "df.select(col(\"name.firstname\").alias(\"fname\"), \\\n",
        "  col(\"name.middlename\").alias(\"mname\"), \\\n",
        "  col(\"name.lastname\").alias(\"lname\"), \\\n",
        "  col(\"dob\"),col(\"gender\"),col(\"salary\")) \\\n",
        "  .printSchema()\n",
        "\n",
        "#5. Using PySpark DataFrame withColumn  To rename nested columns\n",
        "df.withColumn(\"fname\",col(\"name.firstname\")) \\\n",
        "  .withColumn(\"mname\",col(\"name.middlename\")) \\\n",
        "  .withColumn(\"lname\",col(\"name.lastname\")) \\\n",
        "  .drop(\"name\").printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj5DbZNaDUg6",
        "outputId": "a12ef4e8-4225-4014-ca4c-6d27a4277aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- DateOfBirth: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- DateOfBirth: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- fname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- fname: string (nullable = true)\n",
            " |-- mname: string (nullable = true)\n",
            " |-- lname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- fname: string (nullable = true)\n",
            " |-- mname: string (nullable = true)\n",
            " |-- lname: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Using toDF()  To change all columns in a PySpark DataFrame\n",
        "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
        "df.toDF(*newColumns).printSchema()\n"
      ],
      "metadata": {
        "id": "nSRrSRpnH8JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark where() & filter() for efficient data filtering\n",
        "# Consider below example\n",
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField\n",
        "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
        "\n",
        "# Create SparkSession object\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Create data\n",
        "data = [\n",
        "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
        "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
        "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
        "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
        " ]\n",
        "\n",
        "# Create schema\n",
        "schema = StructType([\n",
        "     StructField('name', StructType([\n",
        "        StructField('firstname', StringType(), True),\n",
        "        StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "     ])),\n",
        "     StructField('languages', ArrayType(StringType()), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        " ])\n",
        "\n",
        "# Create dataframe\n",
        "df = spark.createDataFrame(data = data, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "# 1: DataFrame filter() with Column Condition\n",
        "df.filter(df.state == 'OH').show(truncate=False)\n",
        "\n",
        "#2: Using not equal filter condition\n",
        "df.filter(df.state != 'OH').show(truncate=False)\n",
        "df.filter(~(df.state == 'OH')).show(truncate=False)\n",
        "\n",
        "#3: Using col() function\n",
        "df.filter(col(\"state\") == 'OH').show(truncate=False)\n",
        "\n",
        "#4: Filtering with SQL Expression\n",
        "df.filter(\"gender == 'M'\").show(truncate=False)\n",
        "df.filter(\"gender != 'M'\").show(truncate=False)\n",
        "df.filter(\"gender <> 'M'\").show(truncate=False)\n",
        "\n",
        "#5. PySpark Filter with Multiple Conditions\n",
        "df.filter((df.state == 'OH') & (df.gender == 'M')).show(truncate=False)\n",
        "df.filter((col(\"state\") == 'OH') | (col(\"gender\") == 'M')).show(truncate=False)\n",
        "\n",
        "#6. Filter Based on List Values\n",
        "li=[\"OH\",\"CA\",\"DE\"]\n",
        "df.filter(df.state.isin(li)).show(truncate=False)\n",
        "\n",
        "#7. Filter Based on Starts With, Ends With, Contains\n",
        "df.filter(df.state.startswith(\"N\")).show(truncate=False)\n",
        "df.filter(df.state.endswith(\"O\")).show(truncate=False)\n",
        "df.filter(df.state.contains(\"O\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahRiuNTGJtc3",
        "outputId": "46938058-da20-4526-920c-98146f62c694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- languages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+-------------------+------------------+-----+------+\n",
            "|name               |languages         |state|gender|\n",
            "+-------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }     |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}|[CSharp, VB]      |OH   |F     |\n",
            "+-------------------+------------------+-----+------+\n",
            "\n",
            "+-------------------+------------------+-----+------+\n",
            "|name               |languages         |state|gender|\n",
            "+-------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }     |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}|[CSharp, VB]      |OH   |F     |\n",
            "+-------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+----+---------+-----+------+\n",
            "|name|languages|state|gender|\n",
            "+----+---------+-----+------+\n",
            "+----+---------+-----+------+\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Filtering with Regular Expression\n",
        "# Prepare Data\n",
        "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n",
        "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
        "  ]\n",
        "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
        "\n",
        "df2.filter(col(\"name\").like(\"%rose%\")).show(truncate=False)\n",
        "\n",
        "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZeokSIoSmwi",
        "outputId": "dac9ee47-7554-47fc-9044-210c1ce826e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|id |name      |\n",
            "+---+----------+\n",
            "|5  |Rames rose|\n",
            "+---+----------+\n",
            "\n",
            "+---+------------+\n",
            "| id|        name|\n",
            "+---+------------+\n",
            "|  2|Michael Rose|\n",
            "|  4|  Rames Rose|\n",
            "|  5|  Rames rose|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering Array column\n",
        "from pyspark.sql.functions import array_contains\n",
        "df.filter(array_contains(df.languages,\"Java\")) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "# Filtering on Nested Struct columns\n",
        "df.filter(df.name.firstname == 'James') \\\n",
        "  .show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5OU-rSLUSyh",
        "outputId": "159b28c6-345a-416c-dce3-70c85bdf46e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------------+-----+------+\n",
            "|name            |languages         |state|gender|\n",
            "+----------------+------------------+-----+------+\n",
            "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
            "|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n",
            "+----------------+------------------+-----+------+\n",
            "\n",
            "+----------------+------------------+-----+------+\n",
            "|name            |languages         |state|gender|\n",
            "+----------------+------------------+-----+------+\n",
            "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
            "+----------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Distinct to Drop Duplicate Rows\n",
        "#Consider below example\n",
        "\n",
        "# Prepare Data\n",
        "data = [(\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600), \\\n",
        "    (\"Robert\", \"Sales\", 4100), \\\n",
        "    (\"Maria\", \"Finance\", 3000), \\\n",
        "    (\"James\", \"Sales\", 3000), \\\n",
        "    (\"Scott\", \"Finance\", 3300), \\\n",
        "    (\"Jen\", \"Finance\", 3900), \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000), \\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  ]\n",
        "\n",
        "# Create DataFrame\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark_session.createDataFrame(data,columns)\n",
        "df.show(truncate=False)\n",
        "originalDFCount = df.count()\n",
        "print(f\"Original DF Count : {originalDFCount}\")\n",
        "#1:Get Distinct Rows (By Comparing All Columns)\n",
        "df.distinct().show(truncate=False)\n",
        "DistinctCount = df.distinct().count()\n",
        "print(f\"Removed Duplicates DF Count : {DistinctCount}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1_w1RZDZ6ID",
        "outputId": "b6c19867-cb9c-46d8-8865-6049a489b515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Original DF Count : 10\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Michael      |Sales     |4600  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Removed Duplicates DF Count : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Distinct of Selected Multiple Columns\n",
        "disDF = df.select(\"department\",\"salary\").distinct()\n",
        "disDF.show(truncate=False)\n",
        "\n",
        "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
        "dropDisDF.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzffRJhEdRrh",
        "outputId": "bb96be5b-50ab-4feb-f65a-920591409124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|department|salary|\n",
            "+----------+------+\n",
            "|Sales     |4600  |\n",
            "|Sales     |4100  |\n",
            "|Finance   |3000  |\n",
            "|Sales     |3000  |\n",
            "|Finance   |3900  |\n",
            "|Finance   |3300  |\n",
            "|Marketing |2000  |\n",
            "|Marketing |3000  |\n",
            "+----------+------+\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Maria        |Finance   |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Michael      |Sales     |4600  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark orderBy() and sort() explained\n",
        "#Consider below example\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df = spark_session.createDataFrame(simpleData,columns)\n",
        "df.printSchema()\n",
        "\n",
        "#1.DataFrame sorting using the sort() function\n",
        "#df.sort(\"column1\", \"column2\", ascending=[True, False])\n",
        "from pyspark.sql.functions import col\n",
        "df.sort(\"department\",\"state\").show(truncate=False)\n",
        "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
        "\n",
        "#2.DataFrame sorting using orderBy() function\n",
        "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
        "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
        "\n",
        "#3.Sort by Ascending (ASC)\n",
        "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
        "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
        "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
        "\n",
        "#4.Sort by Descending (DESC)\n",
        "df.sort(df.department.desc(),df.state.desc()).show(truncate=False)\n",
        "df.sort(col(\"department\").desc(),col(\"state\").desc()).show(truncate=False)\n",
        "\n",
        "#5. Using Raw SQL\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark_session.sql(\"select * from emp order by department desc\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGaRqTa1fMLg",
        "outputId": "6090ddec-800c-4d75-cd35-ae5844d723ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Groupby Explained with Example\n",
        "#consider below example\n",
        "\n",
        "# Data\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n",
        "\n",
        "# Create DataFrame\n",
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df = spark_session.createDataFrame(simpleData,schema)\n",
        "\n",
        "#1. PySpark groupBy on DataFrame Columns\n",
        "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
        "\n",
        "#2. Using Multiple columns\n",
        "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show(truncate=False)\n",
        "\n",
        "#3. Running more aggregates at a time\n",
        "from pyspark.sql.functions import sum,avg,max\n",
        "df.groupBy(\"department\") \\\n",
        "  .agg(sum(\"salary\").alias(\"Total Salary\"), \\\n",
        "       avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "       sum(\"bonus\").alias(\"Total Bonus\"), \\\n",
        "       max(\"bonus\").alias(\"Maximum Bonus\") \\\n",
        "       ) \\\n",
        "       .show(truncate=False)\n",
        "\n",
        "#4. Using filter on aggregate data\n",
        "df.groupBy(\"department\") \\\n",
        "  .agg(sum(\"salary\").alias(\"Total Salary\"), \\\n",
        "       avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "       sum(\"bonus\").alias(\"Total Bonus\"), \\\n",
        "       max(\"bonus\").alias(\"Maximum Bonus\") \\\n",
        "       ) \\\n",
        "  .where(col(\"Total Bonus\") >= 50000) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "#5. PySpark SQL GROUP BY Query\n",
        "df.createOrReplaceTempView(\"emp\")\n",
        "sql_string = \"\"\"SELECT department,\n",
        "       SUM(salary) AS sum_salary,\n",
        "       AVG(salary) AS avg_salary,\n",
        "       SUM(bonus) AS sum_bonus,\n",
        "       MAX(bonus) AS max_bonus\n",
        "FROM emp\n",
        "GROUP BY department\n",
        "HAVING SUM(bonus) >= 50000\"\"\"\n",
        "\n",
        "\n",
        "spark_session.sql(sql_string).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5MZxOhsKc-q",
        "outputId": "0ca3dec9-4114-4064-f8b3-7d3ffc4c9f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|department|sum(salary)|\n",
            "+----------+-----------+\n",
            "|Sales     |257000     |\n",
            "|Finance   |351000     |\n",
            "|Marketing |171000     |\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----+-----------+----------+\n",
            "|department|state|sum(salary)|sum(bonus)|\n",
            "+----------+-----+-----------+----------+\n",
            "|Sales     |CA   |81000      |23000     |\n",
            "|Finance   |CA   |189000     |47000     |\n",
            "|Sales     |NY   |176000     |30000     |\n",
            "|Finance   |NY   |162000     |34000     |\n",
            "|Marketing |NY   |91000      |21000     |\n",
            "|Marketing |CA   |80000      |18000     |\n",
            "+----------+-----+-----------+----------+\n",
            "\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|department|Total Salary|avg_salary       |Total Bonus|Maximum Bonus|\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|Sales     |257000      |85666.66666666667|53000      |23000        |\n",
            "|Finance   |351000      |87750.0          |81000      |24000        |\n",
            "|Marketing |171000      |85500.0          |39000      |21000        |\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|department|Total Salary|avg_salary       |Total Bonus|Maximum Bonus|\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "|Sales     |257000      |85666.66666666667|53000      |23000        |\n",
            "|Finance   |351000      |87750.0          |81000      |24000        |\n",
            "+----------+------------+-----------------+-----------+-------------+\n",
            "\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
            "|Finance   |351000    |87750.0          |81000    |24000    |\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark Join Types | Join Two DataFrames\n",
        "'''\n",
        "Inner Join: Returns only the rows with matching keys in both DataFrames.\n",
        "Left Join: Returns all rows from the left DataFrame and matching rows from the right DataFrame.\n",
        "Right Join: Returns all rows from the right DataFrame and matching rows from the left DataFrame.\n",
        "Full Outer Join: Returns all rows from both DataFrames, including matching and non-matching rows.\n",
        "Left Semi Join: Returns all rows from the left DataFrame where there is a match in the right DataFrame.\n",
        "Left Anti Join: Returns all rows from the left DataFrame where there is no match in the right DataFrame.\n",
        "'''\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "       (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "       (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "       (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "       (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "       (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "]\n",
        "\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "empDF = spark_session.createDataFrame(emp,empColumns)\n",
        "deptDF = spark_session.createDataFrame(dept,deptColumns)\n",
        "\n",
        "#1: Inner Join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"inner\").show(truncate=False)\n",
        "\n",
        "#2: Left Join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"left\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"leftouter\").show(truncate=False)\n",
        "\n",
        "#3: Righ Join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"right\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"rightouter\").show(truncate=False)\n",
        "\n",
        "#4: Full outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"full\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"fullouter\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"outer\").show(truncate=False)\n",
        "\n",
        "#5: Left Semi join\n",
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"leftsemi\").show(truncate=False)\n",
        "\n",
        "#6: Left Anti join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show(truncate=False)\n",
        "\n",
        "#7: Self join\n",
        "from pyspark.sql.functions import col\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "id": "wJ3O1ix4RfWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebdd171-4755-4b80-fb7f-4af9ebe97cbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|6     |Brown|2              |2010       |50         |      |-1    |\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "\n",
            "+------+--------+---------------+-----------------+\n",
            "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
            "+------+--------+---------------+-----------------+\n",
            "|2     |Rose    |1              |Smith            |\n",
            "|3     |Williams|1              |Smith            |\n",
            "|4     |Jones   |2              |Rose             |\n",
            "|5     |Brown   |2              |Rose             |\n",
            "|6     |Brown   |2              |Rose             |\n",
            "+------+--------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SQL Expression\n",
        "empDF.createOrReplaceTempView(\"emp\")\n",
        "deptDF.createOrReplaceTempView(\"dept\")\n",
        "\n",
        "spark_session.sql(\"select * from emp e, dept d where e.emp_dept_id == d.dept_id\").show(truncate=False)\n",
        "spark_session.sql(\"select * from emp e inner join dept d on e.emp_dept_id == d.dept_id\").show(truncate=False)\n",
        "\n",
        "# PySpark SQL Join on multiple DataFrames\n",
        "# Join on multiple dataFrames\n",
        "df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
        "   .join(df3,df1.id1 == df3.id3,\"inner\")"
      ],
      "metadata": {
        "id": "ZEwyUtNO8mKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Union and UnionAll Explained\n",
        "# union and unionAll - it will merge two dataframes with duplicates\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
        "  ]\n",
        "\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df1 = spark_session.createDataFrame(simpleData,columns)\n",
        "df2 = spark_session.createDataFrame(simpleData2,columns2)\n",
        "\n",
        "unionDF = df1.union(df2)\n",
        "unionDF.show(truncate=False)\n",
        "\n",
        "unionAllDF = df1.unionAll(df2)\n",
        "unionAllDF.show(truncate=False)\n",
        "\n",
        "# Merge without Duplicates\n",
        "\n",
        "MergeDistinctDF = df1.union(df2).distinct()\n",
        "MergeDistinctDF.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4iGVWEyTZL6",
        "outputId": "62f1acea-ab98-459d-a2a4-dcd563c096cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}